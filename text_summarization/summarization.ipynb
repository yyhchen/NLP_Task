{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于 T5 的文本摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1 import related lib\n",
    "\n",
    "- `Seq2SeqTrainer`, `Seq2SeqTrainingArguments` 专门针对 `seq2seq` 任务优化\n",
    "\n",
    "- `seq2seq` 任务的一些特殊要求:\n",
    "    - **自定义损失函数：** 通常是交叉熵函数，但某些情况需要自定义\n",
    "    - **生成任务：** 在评估和预测阶段，Seq2Seq模型需要生成输出序列，这涉及到解码过程，而不仅仅是分类或回归\n",
    "    - **数据处理：** Seq2Seq任务的数据处理可能需要特殊的预处理和后处理步骤，例如对输入和输出序列进行填充和截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2 load datases\n",
    "\n",
    "这里的数据 `nlpcc_2017` 是裁剪过的，原本的实在太大了，为了做实验方便，较少样本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"./nlpcc_2017/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 4900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.train_test_split(100, seed=42)  # 这里加 `seed` 是为了方便后续区分 glm 模型，至于为什么是42，可能是因为《银河系漫游指南》说过宇宙的终极答案是 42\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '组图:黑河边防军人零下30℃户外训练,冰霜沾满眉毛和睫毛,防寒服上满是冰霜。',\n",
       " 'content': '中国军网2014-12-1709:08:0412月16日,黑龙江省军区驻黑河某边防团机动步兵连官兵,冒着-30℃严寒气温进行体能训练,挑战极寒,锻造钢筋铁骨。该连素有“世界冠军的摇篮”之称,曾有5人24人次登上世界军事五项冠军的领奖台。(魏建顺摄)黑龙江省军区驻黑河某边防团机动步兵连官兵冒着-30℃严寒气温进行体能训练驻黑河某边防团机动步兵连官兵严寒中户外训练,防寒服上满是冰霜驻黑河某边防团机动步兵连官兵严寒中户外训练,防寒服上满是冰霜官兵睫毛上都被冻上了冰霜官兵们睫毛上都被冻上了冰霜驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 process data\n",
    "\n",
    "`AutoTokenizer` 加载不了 `Langboat/mengzi-t5-base` 的 `tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5Tokenizer(name_or_path='D:\\CodeLibrary\\huggingface_model\\Langboat\\mengzi-t5-base', vocab_size=32028, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32028: AddedToken(\"<extra_id_99>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32029: AddedToken(\"<extra_id_98>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32030: AddedToken(\"<extra_id_97>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32031: AddedToken(\"<extra_id_96>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32032: AddedToken(\"<extra_id_95>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32033: AddedToken(\"<extra_id_94>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32034: AddedToken(\"<extra_id_93>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32035: AddedToken(\"<extra_id_92>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32036: AddedToken(\"<extra_id_91>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32037: AddedToken(\"<extra_id_90>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32038: AddedToken(\"<extra_id_89>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32039: AddedToken(\"<extra_id_88>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32040: AddedToken(\"<extra_id_87>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32041: AddedToken(\"<extra_id_86>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32042: AddedToken(\"<extra_id_85>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32043: AddedToken(\"<extra_id_84>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32044: AddedToken(\"<extra_id_83>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32045: AddedToken(\"<extra_id_82>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32046: AddedToken(\"<extra_id_81>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32047: AddedToken(\"<extra_id_80>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32048: AddedToken(\"<extra_id_79>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32049: AddedToken(\"<extra_id_78>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32050: AddedToken(\"<extra_id_77>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32051: AddedToken(\"<extra_id_76>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32052: AddedToken(\"<extra_id_75>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32053: AddedToken(\"<extra_id_74>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32054: AddedToken(\"<extra_id_73>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32055: AddedToken(\"<extra_id_72>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32056: AddedToken(\"<extra_id_71>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32057: AddedToken(\"<extra_id_70>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32058: AddedToken(\"<extra_id_69>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32059: AddedToken(\"<extra_id_68>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32060: AddedToken(\"<extra_id_67>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32061: AddedToken(\"<extra_id_66>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32062: AddedToken(\"<extra_id_65>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32063: AddedToken(\"<extra_id_64>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32064: AddedToken(\"<extra_id_63>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32065: AddedToken(\"<extra_id_62>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32066: AddedToken(\"<extra_id_61>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32067: AddedToken(\"<extra_id_60>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32068: AddedToken(\"<extra_id_59>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32069: AddedToken(\"<extra_id_58>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32070: AddedToken(\"<extra_id_57>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32071: AddedToken(\"<extra_id_56>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32072: AddedToken(\"<extra_id_55>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32073: AddedToken(\"<extra_id_54>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32074: AddedToken(\"<extra_id_53>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32075: AddedToken(\"<extra_id_52>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32076: AddedToken(\"<extra_id_51>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32077: AddedToken(\"<extra_id_50>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32078: AddedToken(\"<extra_id_49>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32079: AddedToken(\"<extra_id_48>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32080: AddedToken(\"<extra_id_47>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32081: AddedToken(\"<extra_id_46>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32082: AddedToken(\"<extra_id_45>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32083: AddedToken(\"<extra_id_44>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32084: AddedToken(\"<extra_id_43>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32085: AddedToken(\"<extra_id_42>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32086: AddedToken(\"<extra_id_41>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32087: AddedToken(\"<extra_id_40>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32088: AddedToken(\"<extra_id_39>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32089: AddedToken(\"<extra_id_38>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32090: AddedToken(\"<extra_id_37>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32091: AddedToken(\"<extra_id_36>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32092: AddedToken(\"<extra_id_35>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32093: AddedToken(\"<extra_id_34>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32094: AddedToken(\"<extra_id_33>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32095: AddedToken(\"<extra_id_32>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32096: AddedToken(\"<extra_id_31>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32097: AddedToken(\"<extra_id_30>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32098: AddedToken(\"<extra_id_29>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32099: AddedToken(\"<extra_id_28>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32100: AddedToken(\"<extra_id_27>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32101: AddedToken(\"<extra_id_26>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32102: AddedToken(\"<extra_id_25>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32103: AddedToken(\"<extra_id_24>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32104: AddedToken(\"<extra_id_23>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32105: AddedToken(\"<extra_id_22>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32106: AddedToken(\"<extra_id_21>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32107: AddedToken(\"<extra_id_20>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32108: AddedToken(\"<extra_id_19>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32109: AddedToken(\"<extra_id_18>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32110: AddedToken(\"<extra_id_17>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32111: AddedToken(\"<extra_id_16>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32112: AddedToken(\"<extra_id_15>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32113: AddedToken(\"<extra_id_14>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32114: AddedToken(\"<extra_id_13>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32115: AddedToken(\"<extra_id_12>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32116: AddedToken(\"<extra_id_11>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32117: AddedToken(\"<extra_id_10>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32118: AddedToken(\"<extra_id_9>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32119: AddedToken(\"<extra_id_8>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32120: AddedToken(\"<extra_id_7>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32121: AddedToken(\"<extra_id_6>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32122: AddedToken(\"<extra_id_5>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32123: AddedToken(\"<extra_id_4>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32124: AddedToken(\"<extra_id_3>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32125: AddedToken(\"<extra_id_2>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32126: AddedToken(\"<extra_id_1>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t32127: AddedToken(\"<extra_id_0>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "model_path = r\"D:\\CodeLibrary\\huggingface_model\\Langboat\\mengzi-t5-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seq2seq` 任务中，需要自定义处理函数：\n",
    "- 添加提示词前缀（类似于 prompt）\n",
    "- 将输入（inputs）和输出（labels）序列进行编码\n",
    "- 对输入和输出序列进行填充和截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    contents = [\"生成摘要: \\n\" + e for e in examples['content']]\n",
    "    inputs = tokenizer(examples['content'], max_length=384, truncation=True)\n",
    "    labels = tokenizer(examples['title'], max_length=64, truncation=True)\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc670f6909f341b78f605dcf669f18b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b98520475b742b5aaa153fc1ef65fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中国军网2014-12-1709:08:0412月16日,黑龙江省军区驻黑河某边防团机动步兵连官兵,冒着-30°C严寒气温进行体能训练,挑战极寒,锻造钢筋铁骨。该连素有“世界冠军的摇篮”之称,曾有5人24人次登上世界军事五项冠军的领奖台。(魏建顺摄)黑龙江省军区驻黑河某边防团机动步兵连官兵冒着-30°C严寒气温进行体能训练驻黑河某边防团机动步兵连官兵严寒中户外训练,防寒服上满是冰霜驻黑河某边防团机动步兵连官兵严寒中户外训练,防寒服上满是冰霜官兵睫毛上都被冻上了冰霜官兵们睫毛上都被冻上了冰霜驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过 tokenzier 后的数据都带有 eos token (</s>)\n",
    "tokenizer.decode(tokenized_ds['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'组图:黑河边防军人零下30°C户外训练,冰霜沾满眉毛和睫毛,防寒服上满是冰霜。</s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4 create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step5 create eval func\n",
    "\n",
    "模型的输出是 logits, 评估指标用的是 Rouge，所以需要decode转为 **汉字**  在进行评估指标计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge_chinese import Rouge\n",
    "\n",
    "rouge = Rouge() # 创建评估指标\n",
    "\n",
    "def compute_metric(evalPred):\n",
    "    predictions, labels = evalPred\n",
    "    decode_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # labels中很多 是 -100 的pad token，转换成 pad token id（正数）\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # 将decode后的数据按英文那样 空格 隔开，好后续分词做rouge（貌似不需要这一步也可以计算rouge）\n",
    "    decode_preds = [\" \".join(p) for p in decode_preds]\n",
    "    decode_labels = [\" \".join(l) for l in decode_labels]\n",
    "\n",
    "    # 计算scores(平均分数)\n",
    "    scores = rouge.get_scores(decode_preds, decode_labels, avg=True)\n",
    "\n",
    "    return {\n",
    "        \"rouge-1\": scores['rouge-1']['f'],  # 取 f1 scores\n",
    "        \"rouge-2\": scores['rouge-2']['f'],\n",
    "        \"rouge-l\": scores['rouge-l']['f']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step6 create args\n",
    "\n",
    "`predict_with_generate` 告诉模型在评估和预测时使用生成（generation）的方式来生成输出序列，而不是简单地使用模型的输出层\n",
    "\n",
    "\n",
    "启用功能：\n",
    "- **生成输出序列**：在评估和预测阶段，模型会使用诸如束搜索（beam search）或贪心搜索（greedy search）等生成策略来生成输出序列。这对于像机器翻译、文本摘要等任务非常重要，因为这些任务需要模型生成连贯且有意义的文本。\n",
    "\n",
    "- **计算生成指标：** 启用这个参数后，模型可以计算一些与生成相关的指标，如BLEU、ROUGE等，这些指标通常用于评估生成文本的质量。\n",
    "\n",
    "- **兼容性：** 许多预训练的seq2seq模型（如T5、BART等）在设计时就考虑了生成任务，因此使用predict_with_generate=True 可以确保模型在评估和预测时能够正确地使用这些生成策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summary\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=8,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True # seq2seq 模型一定要加\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step7 create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metric,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step8 model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 12:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.634000</td>\n",
       "      <td>2.330518</td>\n",
       "      <td>0.486443</td>\n",
       "      <td>0.314908</td>\n",
       "      <td>0.408104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=153, training_loss=3.066711416431502, metrics={'train_runtime': 772.5064, 'train_samples_per_second': 6.343, 'train_steps_per_second': 0.198, 'total_flos': 1963761044484096.0, 'train_loss': 3.066711416431502, 'epoch': 0.9991836734693877})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step9 model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '中船重工拟对其相关业务进行调整或涉及公司发行股份;中船重工拟对外投资,涉及定增等交易,涉及公司股票停牌'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"摘要生成:\\n\" + ds[\"test\"][-1][\"content\"], max_length=64, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中国重工拟以持有的动力相关资产进行对外投资,参与中船重工拟打造的动力业务平台公司,将继续停牌。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"test\"][-1][\"title\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
