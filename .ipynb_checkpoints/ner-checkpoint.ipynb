{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e38825-547c-461b-8f22-8dceedb749fd",
   "metadata": {},
   "source": [
    "# å®æˆ˜é¡¹ç›®ä¹‹å‘½åå®ä½“è¯†åˆ«ï¼ˆNLPåŸºç¡€ï¼‰Â Â Â Â \n",
    "<br>Â Â Â \n",
    "\n",
    "## 1âƒ£ï¸ å¯¼å…¥ç›¸å…³åŒ…ï¼Œtensorflowä¼šæŠ¥ä¸€ä¸ªwarningsï¼Œæ•…ç”¨warnings.filterwarningè¿‡æ»¤æ‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6249aaa-cd44-4eaa-9c95-e17ec8ddac5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\CodeLibrary\\\\NLP_Task'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tensorflow\")\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments\n",
    "\n",
    "import os\n",
    "os.getcwd() # æŸ¥çœ‹æ–‡ä»¶å¤¹è·¯å¾„Â "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a56ca-4f52-4907-916f-69953d6463b8",
   "metadata": {},
   "source": [
    "## 2âƒ£ï¸ åŠ è½½äººæ°‘æ—¥æŠ¥æ•°æ®ï¼ˆä¸€èˆ¬éƒ½ä¼šç”¨è¿™ä¸ªæ•°æ®åšnerï¼‰Â Â Â \n",
    "\n",
    "<br>\n",
    "\n",
    "### åˆ©ç”¨hfè”ç½‘åŠ è½½é”™è¯¯\n",
    "```pythonÂ Â Â Â Â Â \n",
    "ner_datastes = load_dataset(\"peoples_daily_ner\", cache_dir=\"./ner/data\")  # ä»hfä¸­ä¸‹è½½æ•°æ®ï¼Œå¹¶ç¼“å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹\n",
    "```\n",
    "**æœ‰å¯èƒ½ä¼šæŠ¥é”™ï¼š** ConnectionError: Couldn't reach 'peoples_daily_ner' on the Hub (ConnectTimeout)Â Â \n",
    "\n",
    "<br>\n",
    "\n",
    "### æœ¬åœ°åŠ è½½é”™è¯¯Â Â Â Â Â Â Â \n",
    "```python\n",
    "ner_datasets = DatasetDict.load_from_disk(\"D:\\\\CodeLibrary\\\\NLP_Task\\\\ner\\\\ner_data\")\n",
    "```\n",
    "**æŠ¥é”™ï¼š** ValueError: Protocol not known: D:\\CodeLibrary\\NLP_Task\\ner_data \n",
    "\n",
    "**å¯èƒ½åŸå› ï¼š** è§£æå­˜å‚¨è·¯å¾„æ—¶ï¼Œfsspecåº“æ— æ³•è¯†åˆ«åè®®ï¼ˆProtocolï¼‰\n",
    "\n",
    "**è§£å†³åŠæ³•ï¼š** åœ¨è·¯å¾„å‰é¢åŠ  `file://` å³å¯ï¼Œ æˆ–è€…æ›´æ–°åº“ `pip install -U datasets fsspec` (è¿™ä¸ªæ²¡è¯•ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07018ba-65d4-410c-983e-08fdd13606ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "# ner_datastes = load_dataset(\"peoples_daily_ner\", cache_dir=\"./data\")\n",
    "ner_datasets = DatasetDict.load_from_disk(\"file://D:\\\\CodeLibrary\\\\NLP_Task\\\\ner_data\")\n",
    "ner_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91a27fc-2440-4c99-83a0-80b2767c2ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['æµ·',\n",
       "  'é’“',\n",
       "  'æ¯”',\n",
       "  'èµ›',\n",
       "  'åœ°',\n",
       "  'ç‚¹',\n",
       "  'åœ¨',\n",
       "  'å¦',\n",
       "  'é—¨',\n",
       "  'ä¸',\n",
       "  'é‡‘',\n",
       "  'é—¨',\n",
       "  'ä¹‹',\n",
       "  'é—´',\n",
       "  'çš„',\n",
       "  'æµ·',\n",
       "  'åŸŸ',\n",
       "  'ã€‚'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b1610-cc0c-4eb6-915e-70de42c05f11",
   "metadata": {},
   "source": [
    "## è·å– ner å‘½åçš„æ ‡ç­¾ï¼Œä¹Ÿå°±è¯´æŸ¥çœ‹äººæ°‘æ—¥æŠ¥æ˜¯å“ªç§neræ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8364c61e-d44f-42da-a268-babe4a803808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_datasets['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00175c-1d3e-4f1a-8626-d14266b876b1",
   "metadata": {},
   "source": [
    "### ä¸Šé¢çš„ç»“æœå¯ä»¥çœ‹åˆ°ï¼Œ'ner_tag' å±æ€§ä¸­çœ‹åˆ°äº† `label` çš„å€¼, æ•…æ ¹æ®å±æ€§åè¿›è¡Œé€çº§è®¿é—®, å³å¯å¾—åˆ° ner çš„ tag åˆ—è¡¨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daad6245-6e15-4b26-9983-4875621bfcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ner_datasets['train'].features['ner_tags'].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8900e6-0f88-473f-b208-ba5fdfa30adf",
   "metadata": {},
   "source": [
    "## 3âƒ£ï¸ æ•°æ®é¢„å¤„ç†ï¼Œè½¬ä¸ºæ¨¡å‹è¾“å…¥çš„å½¢å¼Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf33ecc-ed10-4595-bc52-9ffd728358f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85e9f5-84b2-4530-b7dd-b4d1d41fb18f",
   "metadata": {},
   "source": [
    "### éšä¾¿æ‰¾ä¸€ä¸ªä¾‹å­è¯•ä¸€ä¸‹åˆ†è¯Â Â Â Â Â Â Â Â \n",
    "\n",
    "**éœ€è¦æ³¨æ„çš„æ˜¯ï¼š** å‚æ•° `is_split_into_words=True` æ˜¯è¦æ±‚ `tokenizer` **ä¸ç”¨å†æ¬¡** åˆ†è¯, å› ä¸ºå¾—åˆ°çš„æ•°æ®å·²ç»æ˜¯åˆ†è¯è¿‡äº†çš„ï¼Œè¿™å¯¹äºnerä»»åŠ¡å¾ˆé‡è¦ï¼Œå› ä¸ºæ¨¡å‹è‡ªå¸¦çš„ `tokenizer` å¯èƒ½ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„åˆ†è¯ç»“æœã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a631d0b-5c34-4e80-965f-789c23923442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(ner_datasets[\"train\"][0]['tokens'], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75272d9c-fa4c-496b-bb65-db8fcffb1460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8701, 8572, 117, 8256, 9059, 10716, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tokenizer('hello world, and nothing')  # å¦‚æœè®¾ç½® is_split_into_words=Trueä¼šæŠ¥é”™ï¼Œå› ä¸ºæ²¡æœ‰åˆ†è¯çš„æ•°æ®ä¸æ˜¯ listÂ Â \n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98cd819-5099-42fb-bec3-3d11defd3a67",
   "metadata": {},
   "source": [
    "### `word_ids()` æ–¹æ³•æ˜¯ç”¨äºè·å–æ¯ä¸ª token å¯¹åº”çš„å•è¯ ID; å³ï¼Œæ¯ä¸ª token å±äºå“ªä¸ªå•è¯çš„æ ‡è¯†\n",
    "\n",
    "\n",
    "æ¯”å¦‚è¯´ï¼Œè¿™é‡Œçš„ `None` è¡¨ç¤ºçš„æ˜¯ `[CLS]` å’Œ `[SEP]`, 0 è¡¨ç¤ºçš„æ˜¯ `hello`, ä»¥æ­¤ç±»æ¨ï¼Œå…¶ä¸­ 4 è¡¨ç¤ºçš„åº”è¯¥æ˜¯ `nothing`ï¼ˆè¿™é‡Œnothingåˆ†æˆäº† no å’Œ thing)Â Â Â Â Â Â Â \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99275058-09a9-4e6f-95a5-8fa8269eb76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 4, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8e97f-69a7-4da9-8cd9-c7b0d8c33ec7",
   "metadata": {},
   "source": [
    "## ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œé€šè¿‡ `word_ids` å®ç°æ ‡ç­¾æ˜ å°„Â Â Â Â Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec55e04-cb99-469b-aba0-12ffbd4ea732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(datas):\n",
    "    # ä½¿ç”¨ tokenizer å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œ tokenizationã€‚\n",
    "    # max_length=128 æŒ‡å®šäº†åºåˆ—çš„æœ€å¤§é•¿åº¦ä¸º 128 tokensã€‚\n",
    "    # truncation=True è¡¨ç¤ºå¦‚æœæ–‡æœ¬è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œå°†è¿›è¡Œæˆªæ–­ã€‚\n",
    "    # is_split_into_words=True è¡¨ç¤ºè¾“å…¥çš„æ–‡æœ¬å·²ç»æ˜¯å•è¯çº§åˆ«çš„ï¼Œä¸éœ€è¦å†æ¬¡åˆ†å‰²ã€‚\n",
    "    tokenized_datas = tokenizer(datas['tokens'], max_length=128, truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨å¤„ç†åçš„æ ‡ç­¾ã€‚\n",
    "    labels = []\n",
    "    \n",
    "    # éå†æ¯ä¸ªæ ·æœ¬çš„ NER æ ‡ç­¾ã€‚\n",
    "    for i, label in enumerate(datas['ner_tags']):\n",
    "        \n",
    "        # è·å–å½“å‰æ ·æœ¬çš„ token å¯¹åº”çš„å•è¯ IDã€‚\n",
    "        # batch_index=i ç”¨äºæŒ‡å®šå½“å‰æ­£åœ¨å¤„ç†çš„æ‰¹æ¬¡ä¸­çš„æ ·æœ¬ç´¢å¼•ã€‚\n",
    "        word_ids = tokenized_datas.word_ids(batch_index=i)\n",
    "        \n",
    "        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨å½“å‰æ ·æœ¬å¤„ç†åçš„æ ‡ç­¾ IDã€‚Â Â Â \n",
    "        label_ids = []\n",
    "        \n",
    "        # éå†æ¯ä¸ª token çš„å•è¯ IDã€‚\n",
    "        for word_id in word_ids:\n",
    "            \n",
    "            # å¦‚æœ word_id is Noneï¼Œè¿™æ„å‘³ç€å½“å‰çš„ token æ˜¯ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ [CLS] æˆ– [SEP]ï¼‰ã€‚\n",
    "            # å°†å…¶æ ‡ç­¾è®¾ç½®ä¸º -100ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å¿½ç•¥æ ‡ç­¾å€¼ã€‚\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            # å¦åˆ™ï¼Œå°†å½“å‰ token çš„æ ‡ç­¾è®¾ç½®ä¸ºåŸå§‹æ ‡ç­¾åˆ—è¡¨ label ä¸­å¯¹åº”å•è¯ ID çš„æ ‡ç­¾ã€‚\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        \n",
    "        # å°†å¤„ç†åçš„æ ‡ç­¾ ID åˆ—è¡¨æ·»åŠ åˆ° labels åˆ—è¡¨ä¸­ã€‚\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    # å°†å¤„ç†åçš„æ ‡ç­¾åˆ—è¡¨æ·»åŠ åˆ° tokenized_datas å­—å…¸ä¸­ï¼Œé”®ä¸º 'labels'ã€‚\n",
    "    tokenized_datas['labels'] = labels\n",
    "    \n",
    "    # è¿”å›åŒ…å« tokenization ç»“æœå’Œæ ‡ç­¾çš„æ•°æ®é›†ã€‚\n",
    "    return tokenized_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af72d5ee-708a-4c72-a1ae-cb392b1bb7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8365f44864de4208b730f3be31956d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä½¿ç”¨ map æ–¹æ³•å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ã€‚\n",
    "# batched=True è¡¨ç¤ºå¯¹æ•°æ®é›†è¿›è¡Œæ‰¹æ¬¡å¤„ç†ã€‚\n",
    "tokenized_datas = ner_datasets.map(process_data, batched=True)\n",
    "tokenized_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b44068-409b-401d-9aea-77cf8f1e59e1",
   "metadata": {},
   "source": [
    "### ğŸŒ° è®©æˆ‘ä»¬æ‰¾ä¸ªæ•°æ®æµ‹è¯•ä¸€ä¸‹å‡½æ•° `process_datas`Â Â Â Â Â Â Â Â \n",
    "\n",
    "å†™æˆ [:1] ï¼ˆç­‰ä»·äº[0]ï¼‰å…¶å®ä¹Ÿæ˜¯ä¸€ä¸ªï¼Œå¦‚æœåªå–ä¸€ä¸ªå†™æˆ[0] åˆ™ä¼šæŠ¥é”™ TypeError: 'int' object is not subscriptable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c6fc2b-c575-4e71-a474-182e548e9aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['0'], 'tokens': [['æµ·', 'é’“', 'æ¯”', 'èµ›', 'åœ°', 'ç‚¹', 'åœ¨', 'å¦', 'é—¨', 'ä¸', 'é‡‘', 'é—¨', 'ä¹‹', 'é—´', 'çš„', 'æµ·', 'åŸŸ', 'ã€‚']], 'ner_tags': [[0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ner_datasets['train'][:1])\n",
    "ner_datasets['train'][:1]['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0fecd7-4608-4729-bfec-427fe02655e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1=process_data(ner_datasets['train'][:1])\n",
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eddd01-2eb8-4f72-9fc3-3dbb53c4777a",
   "metadata": {},
   "source": [
    "### æ‰“å°ä¸‹ä¹‹å‰é¢„å¤„ç†å¥½çš„æ•°æ® `tokenized_datas`Â Â \n",
    "\n",
    "å¯ä»¥çœ‹åˆ° å¤šäº†ä¸€ä¸ª `labels` å±æ€§çš„æ•°æ®äº†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849368cb-e168-4810-8ca9-f56c5b88d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['æµ·', 'é’“', 'æ¯”', 'èµ›', 'åœ°', 'ç‚¹', 'åœ¨', 'å¦', 'é—¨', 'ä¸', 'é‡‘', 'é—¨', 'ä¹‹', 'é—´', 'çš„', 'æµ·', 'åŸŸ', 'ã€‚'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datas['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66908f5-503d-4f50-b5c1-c841536de2f1",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ æ•°æ®å¤„ç†å¥½äº†ä¹‹åï¼Œåˆ›å»ºæ¨¡å‹Â Â Â Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6dccfd-3d0f-4cd9-8fcb-caf9fdfef9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('hfl/chinese-macbert-base', num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d340d885-4e93-4c07-a6bc-fec148ff0794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6216712e-a50a-4f5a-9582-b42720b6f2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"hfl/chinese-macbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3335918-5e36-4bcb-8882-cc3fadca8e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8165640-23dd-4f8c-ba4d-39a2f293493a",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ è¯„ä¼°å‡½æ•°Â Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391c862b-95c6-452d-9a65-18eb13fd9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load('D:\\\\CodeLibrary\\\\NLP_Task\\\\ner\\seqeval_metric.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "affad437-7756-4eb0-8bef-32aa12ba0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # å°†idè½¬æ¢ä¸ºåŸå§‹å­—ç¬¦ä¸²ç±»å‹çš„æ ‡ç­¾\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p,l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "\n",
    "    return {\n",
    "        \"f1\": result[\"overall_f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f798f-6315-4ea0-90bd-5f9a56d6bea8",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ é…ç½®è®­ç»ƒå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a6dd01-a2ce-435d-a6b9-75e3ae08d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_ner\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd434b-3cbd-4b08-aba7-bc74f6fc0653",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ åˆ›å»º trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c169f1bb-a233-4de5-a685-210d14220a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForTokenClassification\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datas[\"train\"],\n",
    "    eval_dataset=tokenized_datas[\"validation\"],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe5321-2ebe-42ef-b641-c89bff7f4887",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ model trainingÂ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99991645-968e-4927-972d-717a6a71017a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 02:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.020377</td>\n",
       "      <td>0.938916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=327, training_loss=0.06485472323333087, metrics={'train_runtime': 162.5735, 'train_samples_per_second': 128.342, 'train_steps_per_second': 2.011, 'total_flos': 1317626511207666.0, 'train_loss': 0.06485472323333087, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05af211b-3b43-4b4b-9d9d-988579689aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.02498953975737095,\n",
       " 'eval_f1': 0.9283175752698988,\n",
       " 'eval_runtime': 13.7614,\n",
       " 'eval_samples_per_second': 336.958,\n",
       " 'eval_steps_per_second': 2.689,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datas[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c09f43-1003-4cdd-b799-32f3e146bd3d",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ model prediction\n",
    "\n",
    "`model.config` ä¹‹å‰çœ‹è¿‡äº†ï¼Œå¯ä»¥ç¿»ä¸Šé¢è®°å½•æŸ¥çœ‹ï¼Œå³å¯çŸ¥é“å±æ€§ `id2label` å…¶å®å°±æ˜¯æ ‡ç­¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b3de72b-7c65-4556-8e23-173a63bf0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†ï¼Œè¦æŒ‡å®šid2label\n",
    "model.config.id2label = {idx: label for idx, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae175db0-63ad-40e6-9062-055971e9d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœæ¨¡å‹æ˜¯åŸºäºGPUè®­ç»ƒçš„ï¼Œé‚£ä¹ˆæ¨ç†æ—¶è¦æŒ‡å®šdevice\n",
    "# å¯¹äºNERä»»åŠ¡ï¼Œå¯ä»¥æŒ‡å®šaggregation_strategyä¸ºsimpleï¼Œå¾—åˆ°å…·ä½“çš„å®ä½“çš„ç»“æœï¼Œè€Œä¸æ˜¯tokençš„ç»“æœ\n",
    "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d99938ec-fc00-446b-aec6-ae73241d8dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.96730155,\n",
       "  'word': 'å° æ˜',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9937446,\n",
       "  'word': 'åŒ— äº¬',\n",
       "  'start': 3,\n",
       "  'end': 5}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ner_pipe(\"å°æ˜åœ¨åŒ—äº¬ä¸Šç­\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca59fe-3170-43ec-b6bf-7c5f4e3513ea",
   "metadata": {},
   "source": [
    "### æ ¹æ®startå’Œendå–å®é™…çš„ç»“æœÂ Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca39d7a5-88c8-401e-8eb4-6ad3a11c3e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': ['å°æ˜'], 'LOC': ['åŒ—äº¬']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_result = {}\n",
    "x = \"å°æ˜åœ¨åŒ—äº¬ä¸Šç­\"\n",
    "for r in res:\n",
    "    if r[\"entity_group\"] not in ner_result:\n",
    "        ner_result[r[\"entity_group\"]] = []\n",
    "    ner_result[r[\"entity_group\"]].append(x[r[\"start\"]: r[\"end\"]])\n",
    "\n",
    "ner_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436b99b-caac-441f-b207-abf97904b336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
