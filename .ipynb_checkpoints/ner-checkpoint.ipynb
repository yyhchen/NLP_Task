{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e38825-547c-461b-8f22-8dceedb749fd",
   "metadata": {},
   "source": [
    "# 实战项目之命名实体识别（NLP基础）    \n",
    "<br>   \n",
    "\n",
    "## 1⃣️ 导入相关包，tensorflow会报一个warnings，故用warnings.filterwarning过滤掉\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6249aaa-cd44-4eaa-9c95-e17ec8ddac5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\CodeLibrary\\\\NLP_Task'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tensorflow\")\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments\n",
    "\n",
    "import os\n",
    "os.getcwd() # 查看文件夹路径 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a56ca-4f52-4907-916f-69953d6463b8",
   "metadata": {},
   "source": [
    "## 2⃣️ 加载人民日报数据（一般都会用这个数据做ner）   \n",
    "\n",
    "<br>\n",
    "\n",
    "### 利用hf联网加载错误\n",
    "```python      \n",
    "ner_datastes = load_dataset(\"peoples_daily_ner\", cache_dir=\"./ner/data\")  # 从hf中下载数据，并缓存到指定文件夹\n",
    "```\n",
    "**有可能会报错：** ConnectionError: Couldn't reach 'peoples_daily_ner' on the Hub (ConnectTimeout)  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 本地加载错误       \n",
    "```python\n",
    "ner_datasets = DatasetDict.load_from_disk(\"D:\\\\CodeLibrary\\\\NLP_Task\\\\ner\\\\ner_data\")\n",
    "```\n",
    "**报错：** ValueError: Protocol not known: D:\\CodeLibrary\\NLP_Task\\ner_data \n",
    "\n",
    "**可能原因：** 解析存储路径时，fsspec库无法识别协议（Protocol）\n",
    "\n",
    "**解决办法：** 在路径前面加 `file://` 即可， 或者更新库 `pip install -U datasets fsspec` (这个没试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07018ba-65d4-410c-983e-08fdd13606ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "# ner_datastes = load_dataset(\"peoples_daily_ner\", cache_dir=\"./data\")\n",
    "ner_datasets = DatasetDict.load_from_disk(\"file://D:\\\\CodeLibrary\\\\NLP_Task\\\\ner_data\")\n",
    "ner_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91a27fc-2440-4c99-83a0-80b2767c2ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['海',\n",
       "  '钓',\n",
       "  '比',\n",
       "  '赛',\n",
       "  '地',\n",
       "  '点',\n",
       "  '在',\n",
       "  '厦',\n",
       "  '门',\n",
       "  '与',\n",
       "  '金',\n",
       "  '门',\n",
       "  '之',\n",
       "  '间',\n",
       "  '的',\n",
       "  '海',\n",
       "  '域',\n",
       "  '。'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b1610-cc0c-4eb6-915e-70de42c05f11",
   "metadata": {},
   "source": [
    "## 获取 ner 命名的标签，也就说查看人民日报是哪种ner方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8364c61e-d44f-42da-a268-babe4a803808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_datasets['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00175c-1d3e-4f1a-8626-d14266b876b1",
   "metadata": {},
   "source": [
    "### 上面的结果可以看到，'ner_tag' 属性中看到了 `label` 的值, 故根据属性名进行逐级访问, 即可得到 ner 的 tag 列表\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daad6245-6e15-4b26-9983-4875621bfcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ner_datasets['train'].features['ner_tags'].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8900e6-0f88-473f-b208-ba5fdfa30adf",
   "metadata": {},
   "source": [
    "## 3⃣️ 数据预处理，转为模型输入的形式 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf33ecc-ed10-4595-bc52-9ffd728358f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85e9f5-84b2-4530-b7dd-b4d1d41fb18f",
   "metadata": {},
   "source": [
    "### 随便找一个例子试一下分词        \n",
    "\n",
    "**需要注意的是：** 参数 `is_split_into_words=True` 是要求 `tokenizer` **不用再次** 分词, 因为得到的数据已经是分词过了的，这对于ner任务很重要，因为模型自带的 `tokenizer` 可能不是我们想要的分词结果。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a631d0b-5c34-4e80-965f-789c23923442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(ner_datasets[\"train\"][0]['tokens'], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75272d9c-fa4c-496b-bb65-db8fcffb1460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8701, 8572, 117, 8256, 9059, 10716, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tokenizer('hello world, and nothing')  # 如果设置 is_split_into_words=True会报错，因为没有分词的数据不是 list  \n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98cd819-5099-42fb-bec3-3d11defd3a67",
   "metadata": {},
   "source": [
    "### `word_ids()` 方法是用于获取每个 token 对应的单词 ID; 即，每个 token 属于哪个单词的标识\n",
    "\n",
    "\n",
    "比如说，这里的 `None` 表示的是 `[CLS]` 和 `[SEP]`, 0 表示的是 `hello`, 以此类推，其中 4 表示的应该是 `nothing`（这里nothing分成了 no 和 thing)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99275058-09a9-4e6f-95a5-8fa8269eb76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 4, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8e97f-69a7-4da9-8cd9-c7b0d8c33ec7",
   "metadata": {},
   "source": [
    "## 编写一个函数，通过 `word_ids` 实现标签映射     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec55e04-cb99-469b-aba0-12ffbd4ea732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(datas):\n",
    "    # 使用 tokenizer 对输入文本进行 tokenization。\n",
    "    # max_length=128 指定了序列的最大长度为 128 tokens。\n",
    "    # truncation=True 表示如果文本超过最大长度，将进行截断。\n",
    "    # is_split_into_words=True 表示输入的文本已经是单词级别的，不需要再次分割。\n",
    "    tokenized_datas = tokenizer(datas['tokens'], max_length=128, truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    # 初始化一个列表来存储处理后的标签。\n",
    "    labels = []\n",
    "    \n",
    "    # 遍历每个样本的 NER 标签。\n",
    "    for i, label in enumerate(datas['ner_tags']):\n",
    "        \n",
    "        # 获取当前样本的 token 对应的单词 ID。\n",
    "        # batch_index=i 用于指定当前正在处理的批次中的样本索引。\n",
    "        word_ids = tokenized_datas.word_ids(batch_index=i)\n",
    "        \n",
    "        # 初始化一个列表来存储当前样本处理后的标签 ID。   \n",
    "        label_ids = []\n",
    "        \n",
    "        # 遍历每个 token 的单词 ID。\n",
    "        for word_id in word_ids:\n",
    "            \n",
    "            # 如果 word_id is None，这意味着当前的 token 是特殊标记（如 [CLS] 或 [SEP]）。\n",
    "            # 将其标签设置为 -100，这是一个常用的忽略标签值。\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            # 否则，将当前 token 的标签设置为原始标签列表 label 中对应单词 ID 的标签。\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        \n",
    "        # 将处理后的标签 ID 列表添加到 labels 列表中。\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    # 将处理后的标签列表添加到 tokenized_datas 字典中，键为 'labels'。\n",
    "    tokenized_datas['labels'] = labels\n",
    "    \n",
    "    # 返回包含 tokenization 结果和标签的数据集。\n",
    "    return tokenized_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af72d5ee-708a-4c72-a1ae-cb392b1bb7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8365f44864de4208b730f3be31956d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 map 方法对整个数据集进行预处理。\n",
    "# batched=True 表示对数据集进行批次处理。\n",
    "tokenized_datas = ner_datasets.map(process_data, batched=True)\n",
    "tokenized_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b44068-409b-401d-9aea-77cf8f1e59e1",
   "metadata": {},
   "source": [
    "### 🌰 让我们找个数据测试一下函数 `process_datas`        \n",
    "\n",
    "写成 [:1] （等价于[0]）其实也是一个，如果只取一个写成[0] 则会报错 TypeError: 'int' object is not subscriptable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c6fc2b-c575-4e71-a474-182e548e9aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['0'], 'tokens': [['海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。']], 'ner_tags': [[0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ner_datasets['train'][:1])\n",
    "ner_datasets['train'][:1]['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0fecd7-4608-4729-bfec-427fe02655e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1=process_data(ner_datasets['train'][:1])\n",
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eddd01-2eb8-4f72-9fc3-3dbb53c4777a",
   "metadata": {},
   "source": [
    "### 打印下之前预处理好的数据 `tokenized_datas`  \n",
    "\n",
    "可以看到 多了一个 `labels` 属性的数据了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849368cb-e168-4810-8ca9-f56c5b88d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datas['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66908f5-503d-4f50-b5c1-c841536de2f1",
   "metadata": {},
   "source": [
    "## 4️⃣ 数据处理好了之后，创建模型    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6dccfd-3d0f-4cd9-8fcb-caf9fdfef9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('hfl/chinese-macbert-base', num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d340d885-4e93-4c07-a6bc-fec148ff0794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6216712e-a50a-4f5a-9582-b42720b6f2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"hfl/chinese-macbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3335918-5e36-4bcb-8882-cc3fadca8e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8165640-23dd-4f8c-ba4d-39a2f293493a",
   "metadata": {},
   "source": [
    "## 5️⃣ 评估函数  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391c862b-95c6-452d-9a65-18eb13fd9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load('D:\\\\CodeLibrary\\\\NLP_Task\\\\ner\\seqeval_metric.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "affad437-7756-4eb0-8bef-32aa12ba0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # 将id转换为原始字符串类型的标签\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p,l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "\n",
    "    return {\n",
    "        \"f1\": result[\"overall_f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f798f-6315-4ea0-90bd-5f9a56d6bea8",
   "metadata": {},
   "source": [
    "## 6️⃣ 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a6dd01-a2ce-435d-a6b9-75e3ae08d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_ner\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd434b-3cbd-4b08-aba7-bc74f6fc0653",
   "metadata": {},
   "source": [
    "## 7️⃣ 创建 trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c169f1bb-a233-4de5-a685-210d14220a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForTokenClassification\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datas[\"train\"],\n",
    "    eval_dataset=tokenized_datas[\"validation\"],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe5321-2ebe-42ef-b641-c89bff7f4887",
   "metadata": {},
   "source": [
    "## 8️⃣ model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99991645-968e-4927-972d-717a6a71017a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 02:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.020377</td>\n",
       "      <td>0.938916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=327, training_loss=0.06485472323333087, metrics={'train_runtime': 162.5735, 'train_samples_per_second': 128.342, 'train_steps_per_second': 2.011, 'total_flos': 1317626511207666.0, 'train_loss': 0.06485472323333087, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05af211b-3b43-4b4b-9d9d-988579689aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.02498953975737095,\n",
       " 'eval_f1': 0.9283175752698988,\n",
       " 'eval_runtime': 13.7614,\n",
       " 'eval_samples_per_second': 336.958,\n",
       " 'eval_steps_per_second': 2.689,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datas[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c09f43-1003-4cdd-b799-32f3e146bd3d",
   "metadata": {},
   "source": [
    "## 9️⃣ model prediction\n",
    "\n",
    "`model.config` 之前看过了，可以翻上面记录查看，即可知道属性 `id2label` 其实就是标签\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b3de72b-7c65-4556-8e23-173a63bf0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# 使用pipeline进行推理，要指定id2label\n",
    "model.config.id2label = {idx: label for idx, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae175db0-63ad-40e6-9062-055971e9d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果模型是基于GPU训练的，那么推理时要指定device\n",
    "# 对于NER任务，可以指定aggregation_strategy为simple，得到具体的实体的结果，而不是token的结果\n",
    "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d99938ec-fc00-446b-aec6-ae73241d8dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.96730155,\n",
       "  'word': '小 明',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9937446,\n",
       "  'word': '北 京',\n",
       "  'start': 3,\n",
       "  'end': 5}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ner_pipe(\"小明在北京上班\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca59fe-3170-43ec-b6bf-7c5f4e3513ea",
   "metadata": {},
   "source": [
    "### 根据start和end取实际的结果  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca39d7a5-88c8-401e-8eb4-6ad3a11c3e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': ['小明'], 'LOC': ['北京']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_result = {}\n",
    "x = \"小明在北京上班\"\n",
    "for r in res:\n",
    "    if r[\"entity_group\"] not in ner_result:\n",
    "        ner_result[r[\"entity_group\"]] = []\n",
    "    ner_result[r[\"entity_group\"]].append(x[r[\"start\"]: r[\"end\"]])\n",
    "\n",
    "ner_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436b99b-caac-441f-b207-abf97904b336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
