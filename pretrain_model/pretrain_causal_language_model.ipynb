{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预训练模型\n",
    "\n",
    "**causal language model** pretrain case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1 import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, BloomForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2 load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load_from_disk(\"./wiki_cn_filtered/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'completion'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'wikipedia.zh2307',\n",
       " 'completion': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学的博物馆，馆长是锺明善。\\n历史\\n2004年9月20日开始筹建，2013年4月8日正式建成开馆，位于西安交通大学兴庆校区陕西省西安市咸宁西路28号。建筑面积6,800平米，展厅面积4,500平米，馆藏文物4,900余件。包括历代艺术文物馆、碑石书法馆、西部农民画馆、邢良坤陶瓷艺术馆、陕西秦腔博物馆和书画展厅共五馆一厅。\\n营业时间\\n* 周一至周六：上午九点至十二点，下午一点至五点\\n* 周日闭馆\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:\\CodeLibrary\\huggingface_model\\Langboat\\bloom-389m-zh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def process_func(examples):\n",
    "    contents = [e + tokenizer.eos_token for e in examples[\"completion\"]]    # 加上 [EOS] 结束符\n",
    "    return tokenizer(contents, max_length=384, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a79547d7a648748e3241734602360b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))   # causal model， mlm=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'input_ids': tensor([[    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3, 13110, 34800,\n",
       "          13535,   916, 33156,    10,   256,   576,   387,   479,   681,  5453,\n",
       "          10955,   915, 24124,  5317, 13110,  6573, 20757, 13535,   355,  5358,\n",
       "           1490,   583, 28056,  1407,  3855,   671,  6113,   189,  6732,  4302,\n",
       "           9488,  3434,  6900,  1322,   355, 37336,  9825,  4608, 13461,  1359,\n",
       "           5358,   355,  5317, 13110, 34800,  4433,  7189, 25722, 29747, 13110,\n",
       "           1498, 12047,  6347, 23563,  2139,  2066,   420, 29288,    25,    15,\n",
       "           7635, 39288,   355,  1484,  5835,  6272,    23,    15,  4180, 39288,\n",
       "            355,  5358,  4516, 11621,    23,    15, 10641,  4887,  1712,   420,\n",
       "           2450, 31163,  8085, 11621,  5358,   553,  9888,  2731, 21335,  5358,\n",
       "            553,  9876, 14011,  4434,  5358,   553, 21484,  4514, 17170, 25871,\n",
       "           8085,  5358,   553, 17489,  6945, 11097, 13535,   641, 33623,  1484,\n",
       "           5835,  1689,  2063,  5358,   569,  5835,   671, 16225,  3422,   189,\n",
       "             13, 23158, 27894, 33227,  1022, 11396,  3347,  1813,  1504,  6566,\n",
       "           1813,   355,  9155,  8633,  1504,  2063,  1813,   189,    13, 23158,\n",
       "            813,  7817,  5358,     2],\n",
       "         [  586, 11835,   739,   355,  8417,  2300,   916, 16892,  5077,   580,\n",
       "          15549,  1434,   996,   307,   387,   355,  1997,  8236,   775,  8417,\n",
       "           2152,  6496, 12176,  3728,  7692,  1503,  1528,  1014, 17887, 18001,\n",
       "           3250,   355,  6250,  3896,  2670, 20882, 16080, 14005,  3993,  1503,\n",
       "          12295,  8930,  3250,   420,  4208,  4326,  5367,  8417,  2152,  4632,\n",
       "          37591,   355,  1379,  8417,  2300, 32824,  3250,  9015, 18945, 16714,\n",
       "           5908, 13551, 30330, 23756,  2152, 15976,   657,  5923,  8417,   586,\n",
       "          16080,  1528,  8941,  5917, 14035,  5895, 14092,  4353, 29445,   355,\n",
       "           8790, 21595,  1450, 15911, 31116, 10345, 29940, 10874,  1125,  4829,\n",
       "          16080,  7093, 22939,   737,   262,   387,    72,   272,   831,   455,\n",
       "             72,   915,  8860, 20725,  1934,  1084,  5478,   420,  4415,  8417,\n",
       "          26263, 12726,   553, 10875, 34820,   355,  1266,  5498,   586, 14907,\n",
       "          32795, 11835,   904, 19934,  1528, 19531, 20517,  1349, 19472, 28879,\n",
       "            671,  8417, 26263, 17020,  5963, 22388, 11900, 12669, 13240,  1042,\n",
       "           9783,   355,  7242,   714,  1806,   775,  6500,   355, 11526, 10185,\n",
       "           1293,  1665, 15984,  7092,  1617,  8417,  2300,   420, 19972, 25622,\n",
       "          10875, 17500, 26523,  2391,  8417,  2300,   355,  6751,  2836, 13539,\n",
       "           8247,   373, 30201,  5498,   420,  8417,  2300,   586,  8523, 19358,\n",
       "           1298, 12176, 30939, 10739,   964,  4318, 10875,   420, 11900, 16080,\n",
       "            904,  9783,   355, 22464,  9658,   355,  8417,  2300, 13561,  2054,\n",
       "           4983,  4829, 30800,  7262,   420, 11394,  8417, 35143, 11937, 15682,\n",
       "           8417,  2300,  7283, 10875,   355,  1016,  4179,  5039, 14027, 26215,\n",
       "          26835,   671, 15095,   189,  1165, 15095, 11900,  6184,  1125,  3244,\n",
       "           3687,   622,  8785,  1121,   891, 13765,   671, 10199,   189,    13,\n",
       "            210,  6940,  3728,  9552,  6082,  8417,  2300,   916,  4375,   714,\n",
       "           3679,  1806, 10567,   915,   189,    13,   210, 16131, 11835,  8417,\n",
       "           2300,   189,    13, 24075,  3728,  8417,  2300,   916,  4829,  3687,\n",
       "           9000, 27689,  8417,  2300,  1300, 11243,  2062, 28431, 27689, 11835,\n",
       "           8417,  2300, 13224,  4829,  3687, 15964,   915,   189,    13, 27340,\n",
       "          11835,  8417,  2300,   189,    13, 27340,  3982,  3728,  8417,  2300,\n",
       "            916,  4375,  2450,  3272,  1234, 19083,   553,  3512,  3121,  1728,\n",
       "            641,  3092,  2113,  7843,   915,   189,    13,   210, 15402,  8417,\n",
       "           2300,   189,    13, 10057,   108, 12693, 14624, 29379,  4719,  6533,\n",
       "            739,   916, 16148, 10981, 21350,  9067,  1203,  8931,  1258, 11835,\n",
       "           4719,  6533,   739, 20393,   189,    13,   210, 19546,  1517, 11835,\n",
       "           8417,  2300,   189,    13,   210, 23928,   168,   117,   245,  6279,\n",
       "            114,   240,   170,   100,   124,   168,   117,   228,  6279,   100,\n",
       "            124,   168,   117,   228,   171,   238,   224, 41356,   236, 24175,\n",
       "          11082, 10981, 21350,  9067]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 13110, 34800,\n",
       "          13535,   916, 33156,    10,   256,   576,   387,   479,   681,  5453,\n",
       "          10955,   915, 24124,  5317, 13110,  6573, 20757, 13535,   355,  5358,\n",
       "           1490,   583, 28056,  1407,  3855,   671,  6113,   189,  6732,  4302,\n",
       "           9488,  3434,  6900,  1322,   355, 37336,  9825,  4608, 13461,  1359,\n",
       "           5358,   355,  5317, 13110, 34800,  4433,  7189, 25722, 29747, 13110,\n",
       "           1498, 12047,  6347, 23563,  2139,  2066,   420, 29288,    25,    15,\n",
       "           7635, 39288,   355,  1484,  5835,  6272,    23,    15,  4180, 39288,\n",
       "            355,  5358,  4516, 11621,    23,    15, 10641,  4887,  1712,   420,\n",
       "           2450, 31163,  8085, 11621,  5358,   553,  9888,  2731, 21335,  5358,\n",
       "            553,  9876, 14011,  4434,  5358,   553, 21484,  4514, 17170, 25871,\n",
       "           8085,  5358,   553, 17489,  6945, 11097, 13535,   641, 33623,  1484,\n",
       "           5835,  1689,  2063,  5358,   569,  5835,   671, 16225,  3422,   189,\n",
       "             13, 23158, 27894, 33227,  1022, 11396,  3347,  1813,  1504,  6566,\n",
       "           1813,   355,  9155,  8633,  1504,  2063,  1813,   189,    13, 23158,\n",
       "            813,  7817,  5358,     2],\n",
       "         [  586, 11835,   739,   355,  8417,  2300,   916, 16892,  5077,   580,\n",
       "          15549,  1434,   996,   307,   387,   355,  1997,  8236,   775,  8417,\n",
       "           2152,  6496, 12176,  3728,  7692,  1503,  1528,  1014, 17887, 18001,\n",
       "           3250,   355,  6250,  3896,  2670, 20882, 16080, 14005,  3993,  1503,\n",
       "          12295,  8930,  3250,   420,  4208,  4326,  5367,  8417,  2152,  4632,\n",
       "          37591,   355,  1379,  8417,  2300, 32824,  3250,  9015, 18945, 16714,\n",
       "           5908, 13551, 30330, 23756,  2152, 15976,   657,  5923,  8417,   586,\n",
       "          16080,  1528,  8941,  5917, 14035,  5895, 14092,  4353, 29445,   355,\n",
       "           8790, 21595,  1450, 15911, 31116, 10345, 29940, 10874,  1125,  4829,\n",
       "          16080,  7093, 22939,   737,   262,   387,    72,   272,   831,   455,\n",
       "             72,   915,  8860, 20725,  1934,  1084,  5478,   420,  4415,  8417,\n",
       "          26263, 12726,   553, 10875, 34820,   355,  1266,  5498,   586, 14907,\n",
       "          32795, 11835,   904, 19934,  1528, 19531, 20517,  1349, 19472, 28879,\n",
       "            671,  8417, 26263, 17020,  5963, 22388, 11900, 12669, 13240,  1042,\n",
       "           9783,   355,  7242,   714,  1806,   775,  6500,   355, 11526, 10185,\n",
       "           1293,  1665, 15984,  7092,  1617,  8417,  2300,   420, 19972, 25622,\n",
       "          10875, 17500, 26523,  2391,  8417,  2300,   355,  6751,  2836, 13539,\n",
       "           8247,   373, 30201,  5498,   420,  8417,  2300,   586,  8523, 19358,\n",
       "           1298, 12176, 30939, 10739,   964,  4318, 10875,   420, 11900, 16080,\n",
       "            904,  9783,   355, 22464,  9658,   355,  8417,  2300, 13561,  2054,\n",
       "           4983,  4829, 30800,  7262,   420, 11394,  8417, 35143, 11937, 15682,\n",
       "           8417,  2300,  7283, 10875,   355,  1016,  4179,  5039, 14027, 26215,\n",
       "          26835,   671, 15095,   189,  1165, 15095, 11900,  6184,  1125,  3244,\n",
       "           3687,   622,  8785,  1121,   891, 13765,   671, 10199,   189,    13,\n",
       "            210,  6940,  3728,  9552,  6082,  8417,  2300,   916,  4375,   714,\n",
       "           3679,  1806, 10567,   915,   189,    13,   210, 16131, 11835,  8417,\n",
       "           2300,   189,    13, 24075,  3728,  8417,  2300,   916,  4829,  3687,\n",
       "           9000, 27689,  8417,  2300,  1300, 11243,  2062, 28431, 27689, 11835,\n",
       "           8417,  2300, 13224,  4829,  3687, 15964,   915,   189,    13, 27340,\n",
       "          11835,  8417,  2300,   189,    13, 27340,  3982,  3728,  8417,  2300,\n",
       "            916,  4375,  2450,  3272,  1234, 19083,   553,  3512,  3121,  1728,\n",
       "            641,  3092,  2113,  7843,   915,   189,    13,   210, 15402,  8417,\n",
       "           2300,   189,    13, 10057,   108, 12693, 14624, 29379,  4719,  6533,\n",
       "            739,   916, 16148, 10981, 21350,  9067,  1203,  8931,  1258, 11835,\n",
       "           4719,  6533,   739, 20393,   189,    13,   210, 19546,  1517, 11835,\n",
       "           8417,  2300,   189,    13,   210, 23928,   168,   117,   245,  6279,\n",
       "            114,   240,   170,   100,   124,   168,   117,   228,  6279,   100,\n",
       "            124,   168,   117,   228,   171,   238,   224, 41356,   236, 24175,\n",
       "          11082, 10981, 21350,  9067]])})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4 create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step5 config train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./causal_lm\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step6 create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step7 model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 3:04:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.963200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.638200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.563600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.517600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.554900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.405600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.457800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=312, training_loss=3.5985299394680905, metrics={'train_runtime': 11076.6577, 'train_samples_per_second': 0.903, 'train_steps_per_second': 0.028, 'total_flos': 6092709626314752.0, 'train_loss': 3.5985299394680905, 'epoch': 0.9984})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step8 model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学大唐芙蓉池遗址遗址公园内的西安交通大学博物馆，由长安古建博物馆和中国西安城墙博物馆相结合的开放博物馆。\\n历史\\n根据西安城市规划局的西安城市规划，西安交通大学大唐芙蓉池遗址遗址公园内修建了西安交通大学博物馆。博物馆位于大唐芙蓉池遗址内，建筑面积近4万平米，为西安市内规模最大的大型博物馆。西安城规建设时，将西安交通大学博物馆作为文物建筑纳入西安市历史文化名城，被定位为西安市历史文化名城核心区之一\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do_sample参数\n",
    "pipe(\"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常顺利，近日，游戏相关产业发展，而且从2016年开始，中国游戏研发已经非常成功。\\n由于游戏产业比较发达、经济效益可观，故而越来越多的人加入到了游戏的浪潮中去。而对于这游戏产业来讲，并不是只有中国游戏公司可以参与该游戏产业的竞争，比如欧美的一些大型游戏公司也有参与中国游戏公司研发的游戏。但是由于这些游戏公司开发了比较好的题材，所以大部分的中小企业并没有参与中国游戏产业的竞争。并且由于现在的中小企业在游戏开发和营销方面比'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常\", max_length=128, do_sample=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
