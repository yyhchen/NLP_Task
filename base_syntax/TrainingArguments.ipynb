{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9a1caa-470d-4a7d-b1a2-f8f84ca7aecb",
   "metadata": {},
   "source": [
    "# 利用pytorch 手动实现 transformers库中保存模型的功能\n",
    "\n",
    "首先准备一些前置的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be7a1af-01ab-4be0-ac89-bfc92762a3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0be093a455485ca2148ce5e58c287d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c142f21a73cb4dac8a0a832e7632c875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/777 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "f1_score = evaluate.load('f1')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-macbert-large', trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-macbert-large')\n",
    "\n",
    "\n",
    "dataset = load_dataset('csv', data_files='D:\\\\CodeLibrary\\\\NLP_Task\\\\classification_demo\\\\ChnSentiCorp_htl_all.csv', split='train')\n",
    "dataset = dataset.filter(lambda x : x['review'] is not None)\n",
    "split_datasets = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "def process_datasets(examples):\n",
    "    tokenized_examples = tokenizer(examples['review'], max_length=32, truncation=True, padding=True)\n",
    "    tokenized_examples['label'] = examples['label'] # 重新添加 ['label']\n",
    "    return tokenized_examples\n",
    "\n",
    "# 带有 remove_columns 的 feature栏 少了 ['review'], 本质上 ['label'] 也被删除了，只是重新添加回去了\n",
    "tokenized_datasets = split_datasets.map(process_datasets, batched=True, remove_columns=split_datasets['test'].column_names)\n",
    "\n",
    "trainset, validset = tokenized_datasets['train'], tokenized_datasets['test']\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "test_loader = DataLoader(validset, batch_size=32, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d21d74-82b0-44ff-b2d1-94e818dfdecb",
   "metadata": {},
   "source": [
    "# transformers 库提供了 `TrainingArguments` 封装了训练过程中的许多功能，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dcef1f-cadd-4a55-a0c7-fc3b28fa3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='./temps', ## 模型保存地址\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    gradient_checkpointing=True,\n",
    "    optim='adafactor',\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=3,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model='f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e494818-0ef5-4fb5-9c4f-90b02f228ad4",
   "metadata": {},
   "source": [
    "# 如果用Pytorch实现 参数\n",
    "```python\n",
    "output_dir='./temps',\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c369728c-eb23-412b-bd58-d80a5669c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 先定义目录\n",
    "output_dir='./temps'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "assert os.path.exists(output_dir), f\"output directory '{output_dir}' does not exist\"\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, f1):\n",
    "    checkpoint_path = os.path.join(output_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    global best_f1\n",
    "    if f1['f1'] > best_f1:\n",
    "        best_f1 = f1['f1']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'f1_score': f1['f1']\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135745e0-4f44-490d-9fe9-d507d9b45062",
   "metadata": {},
   "source": [
    "# 在训练函数中增加 `save_checkpoint` 函数 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5431fc3-ab35-4a5c-b6f0-a9d65488176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=3, log_step=100):\n",
    "    global_step=0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = len(tokenized_datasets['train'])\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for batch in train_loader:\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            prediction = torch.argmax(output.logits, dim=-1)\n",
    "            # print('predictions', prediction)\n",
    "            # print('labels', batch['labels'])\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            if global_step % log_step == 0:\n",
    "                print(f\"{epoch}, global_step:{global_step}, loss:{output.loss.item()}\")\n",
    "            global_step += 1\n",
    "            all_predictions.extend(prediction.cpu().tolist())\n",
    "            all_labels.extend(batch['labels'].cpu().tolist())\n",
    "        acc = accuracy.compute(predictions=all_predictions, references=all_labels)\n",
    "        f1 = f1_score.compute(predictions=all_predictions, references=all_labels)\n",
    "        save_checkpoint(epoch, model, optimizer, f1)\n",
    "        print(f\"acc: {acc}, f1:{f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd43dd29-75a7-4677-a13c-424c7790a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, global_step:0, loss:0.26602548360824585\n",
      "0, global_step:100, loss:0.1833508163690567\n",
      "0, global_step:200, loss:0.29030925035476685\n",
      "Checkpoint saved to ./temps\\checkpoint_epoch_0.pt\n",
      "acc: {'accuracy': 0.8942472810532341}, f1:{'f1': 0.924306053467172}\n",
      "1, global_step:300, loss:0.11631196737289429\n",
      "1, global_step:400, loss:0.06651370972394943\n",
      "Checkpoint saved to ./temps\\checkpoint_epoch_1.pt\n",
      "acc: {'accuracy': 0.9384659416141957}, f1:{'f1': 0.9557886078552333}\n",
      "2, global_step:500, loss:0.026110773906111717\n",
      "2, global_step:600, loss:0.09740591049194336\n",
      "Checkpoint saved to ./temps\\checkpoint_epoch_2.pt\n",
      "acc: {'accuracy': 0.9675157412707499}, f1:{'f1': 0.9765374677002584}\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302d1b8-5b53-4a6b-916b-23f45fcd8726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
