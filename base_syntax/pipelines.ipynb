{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e56eb58-d669-49c7-ba34-cb29a86a1c6c",
   "metadata": {},
   "source": [
    "# transformers库中 Pipeline的使用demo   \n",
    "\n",
    "可以使用 `inspect` 来查看 `pipeline` 的源码来获取该函数输入参数要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499fba1c-a775-4073-b182-13c3fd67f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b461e4e-b656-49d2-bde2-d35416797270",
   "metadata": {},
   "source": [
    "## 查看pipeline支持的任务类型   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee59842-0c1e-4e20-8c2a-b5ceaa189522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "# print(SUPPORTED_TASKS.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75eb47-6008-4ca4-8fb7-8f64843f42e0",
   "metadata": {},
   "source": [
    "### 上面的结果是非常乱的（kv形式数据），我们使用for循环来遍历出清晰客观的形式  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502c2b3a-fb26-44d5-a00c-cb47e5d7773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-classification ===> {'impl': <class 'transformers.pipelines.audio_classification.AudioClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForAudioClassification'>,), 'default': {'model': {'pt': ('superb/wav2vec2-base-superb-ks', '372e048')}}, 'type': 'audio'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "automatic-speech-recognition ===> {'impl': <class 'transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForCTC'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSpeechSeq2Seq'>), 'default': {'model': {'pt': ('facebook/wav2vec2-base-960h', '55bb623')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "text-to-audio ===> {'impl': <class 'transformers.pipelines.text_to_audio.TextToAudioPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForTextToWaveform'>, <class 'transformers.models.auto.modeling_auto.AutoModelForTextToSpectrogram'>), 'default': {'model': {'pt': ('suno/bark-small', '645cfba')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature-extraction ===> {'impl': <class 'transformers.pipelines.feature_extraction.FeatureExtractionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModel'>,), 'default': {'model': {'pt': ('distilbert/distilbert-base-cased', '935ac13'), 'tf': ('distilbert/distilbert-base-cased', '935ac13')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "text-classification ===> {'impl': <class 'transformers.pipelines.text_classification.TextClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>,), 'default': {'model': {'pt': ('distilbert/distilbert-base-uncased-finetuned-sst-2-english', 'af0f99b'), 'tf': ('distilbert/distilbert-base-uncased-finetuned-sst-2-english', 'af0f99b')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "token-classification ===> {'impl': <class 'transformers.pipelines.token_classification.TokenClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>,), 'default': {'model': {'pt': ('dbmdz/bert-large-cased-finetuned-conll03-english', 'f2482bf'), 'tf': ('dbmdz/bert-large-cased-finetuned-conll03-english', 'f2482bf')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "question-answering ===> {'impl': <class 'transformers.pipelines.question_answering.QuestionAnsweringPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForQuestionAnswering'>,), 'default': {'model': {'pt': ('distilbert/distilbert-base-cased-distilled-squad', '626af31'), 'tf': ('distilbert/distilbert-base-cased-distilled-squad', '626af31')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "table-question-answering ===> {'impl': <class 'transformers.pipelines.table_question_answering.TableQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForTableQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('google/tapas-base-finetuned-wtq', '69ceee2'), 'tf': ('google/tapas-base-finetuned-wtq', '69ceee2')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "visual-question-answering ===> {'impl': <class 'transformers.pipelines.visual_question_answering.VisualQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVisualQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('dandelin/vilt-b32-finetuned-vqa', '4355f59')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "document-question-answering ===> {'impl': <class 'transformers.pipelines.document_question_answering.DocumentQuestionAnsweringPipeline'>, 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForDocumentQuestionAnswering'>,), 'tf': (), 'default': {'model': {'pt': ('impira/layoutlm-document-qa', '52e01b3')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "fill-mask ===> {'impl': <class 'transformers.pipelines.fill_mask.FillMaskPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForMaskedLM'>,), 'default': {'model': {'pt': ('distilbert/distilroberta-base', 'ec58a5b'), 'tf': ('distilbert/distilroberta-base', 'ec58a5b')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "summarization ===> {'impl': <class 'transformers.pipelines.text2text_generation.SummarizationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {'model': {'pt': ('sshleifer/distilbart-cnn-12-6', 'a4f8f3e'), 'tf': ('google-t5/t5-small', 'd769bba')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "translation ===> {'impl': <class 'transformers.pipelines.text2text_generation.TranslationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {('en', 'fr'): {'model': {'pt': ('google-t5/t5-base', '686f1db'), 'tf': ('google-t5/t5-base', '686f1db')}}, ('en', 'de'): {'model': {'pt': ('google-t5/t5-base', '686f1db'), 'tf': ('google-t5/t5-base', '686f1db')}}, ('en', 'ro'): {'model': {'pt': ('google-t5/t5-base', '686f1db'), 'tf': ('google-t5/t5-base', '686f1db')}}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "text2text-generation ===> {'impl': <class 'transformers.pipelines.text2text_generation.Text2TextGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,), 'default': {'model': {'pt': ('google-t5/t5-base', '686f1db'), 'tf': ('google-t5/t5-base', '686f1db')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "text-generation ===> {'impl': <class 'transformers.pipelines.text_generation.TextGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,), 'default': {'model': {'pt': ('openai-community/gpt2', '6c0e608'), 'tf': ('openai-community/gpt2', '6c0e608')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "zero-shot-classification ===> {'impl': <class 'transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>,), 'default': {'model': {'pt': ('facebook/bart-large-mnli', 'c626438'), 'tf': ('FacebookAI/roberta-large-mnli', '130fb28')}, 'config': {'pt': ('facebook/bart-large-mnli', 'c626438'), 'tf': ('FacebookAI/roberta-large-mnli', '130fb28')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "zero-shot-image-classification ===> {'impl': <class 'transformers.pipelines.zero_shot_image_classification.ZeroShotImageClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForZeroShotImageClassification'>,), 'default': {'model': {'pt': ('openai/clip-vit-base-patch32', 'f4881ba'), 'tf': ('openai/clip-vit-base-patch32', 'f4881ba')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "zero-shot-audio-classification ===> {'impl': <class 'transformers.pipelines.zero_shot_audio_classification.ZeroShotAudioClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModel'>,), 'default': {'model': {'pt': ('laion/clap-htsat-fused', '973b6e5')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "conversational ===> {'impl': <class 'transformers.pipelines.conversational.ConversationalPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>), 'default': {'model': {'pt': ('microsoft/DialoGPT-medium', '8bada3b'), 'tf': ('microsoft/DialoGPT-medium', '8bada3b')}}, 'type': 'text'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "image-classification ===> {'impl': <class 'transformers.pipelines.image_classification.ImageClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForImageClassification'>,), 'default': {'model': {'pt': ('google/vit-base-patch16-224', '5dca96d'), 'tf': ('google/vit-base-patch16-224', '5dca96d')}}, 'type': 'image'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "image-feature-extraction ===> {'impl': <class 'transformers.pipelines.image_feature_extraction.ImageFeatureExtractionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModel'>,), 'default': {'model': {'pt': ('google/vit-base-patch16-224', '3f49326'), 'tf': ('google/vit-base-patch16-224', '3f49326')}}, 'type': 'image'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "image-segmentation ===> {'impl': <class 'transformers.pipelines.image_segmentation.ImageSegmentationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>), 'default': {'model': {'pt': ('facebook/detr-resnet-50-panoptic', 'fc15262')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "image-to-text ===> {'impl': <class 'transformers.pipelines.image_to_text.ImageToTextPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVision2Seq'>,), 'default': {'model': {'pt': ('ydshieh/vit-gpt2-coco-en', '65636df'), 'tf': ('ydshieh/vit-gpt2-coco-en', '65636df')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "object-detection ===> {'impl': <class 'transformers.pipelines.object_detection.ObjectDetectionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForObjectDetection'>,), 'default': {'model': {'pt': ('facebook/detr-resnet-50', '2729413')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "zero-shot-object-detection ===> {'impl': <class 'transformers.pipelines.zero_shot_object_detection.ZeroShotObjectDetectionPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForZeroShotObjectDetection'>,), 'default': {'model': {'pt': ('google/owlvit-base-patch32', '17740e1')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "depth-estimation ===> {'impl': <class 'transformers.pipelines.depth_estimation.DepthEstimationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForDepthEstimation'>,), 'default': {'model': {'pt': ('Intel/dpt-large', 'e93beec')}}, 'type': 'image'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "video-classification ===> {'impl': <class 'transformers.pipelines.video_classification.VideoClassificationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForVideoClassification'>,), 'default': {'model': {'pt': ('MCG-NJU/videomae-base-finetuned-kinetics', '4800870')}}, 'type': 'video'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "mask-generation ===> {'impl': <class 'transformers.pipelines.mask_generation.MaskGenerationPipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForMaskGeneration'>,), 'default': {'model': {'pt': ('facebook/sam-vit-huge', '997b15')}}, 'type': 'multimodal'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "image-to-image ===> {'impl': <class 'transformers.pipelines.image_to_image.ImageToImagePipeline'>, 'tf': (), 'pt': (<class 'transformers.models.auto.modeling_auto.AutoModelForImageToImage'>,), 'default': {'model': {'pt': ('caidas/swin2SR-classical-sr-x2-64', '4aaedcb')}}, 'type': 'image'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k, v in SUPPORTED_TASKS.items():\n",
    "    print(k, \"===>\", v)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d7ed86-c981-4fe6-acef-a42c16b05d0f",
   "metadata": {},
   "source": [
    "### 根据不同的任务类型创建pipeline，需要注意的是❗️ 支持的任务和填写的参数 task是不一样的\n",
    "\n",
    "在 `pipeline` 源码中，可以看到第一个参数 `task` 填写应该如下:\n",
    "\n",
    "task (`str`):\n",
    "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
    "\n",
    "            - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
    "            - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
    "            - `\"conversational\"`: will return a [`ConversationalPipeline`].\n",
    "            - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
    "            - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
    "            - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
    "            - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
    "            - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
    "            - `\"image-feature-extraction\"`: will return an [`ImageFeatureExtractionPipeline`].\n",
    "            - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
    "            - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n",
    "            - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
    "            - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
    "            - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
    "            - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
    "            - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
    "            - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
    "            - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
    "            - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
    "              [`TextClassificationPipeline`].\n",
    "            - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
    "            - `\"text-to-audio\"` (alias `\"text-to-speech\"` available): will return a [`TextToAudioPipeline`]:.\n",
    "            - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
    "            - `\"translation\"`: will return a [`TranslationPipeline`].\n",
    "            - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
    "            - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
    "            - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
    "            - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
    "            - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
    "            - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
    "            - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd5e3c-0e81-4c1a-b37c-1295db9e1e83",
   "metadata": {},
   "source": [
    "### 比如根据pipeline创建 summarization 任务 （默认都是英文～用中文答案也是英文）     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "可以看到，如果没有指定 model 直接执行 pipline 会出现一些警告      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1769290-7726-40a0-b851-003dd3550622",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00b10bb5-a6c9-4ba2-bc72-a156676bcde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I was born in the country, my family lived there before I was 6 years old. Then my parents moved to the city for their work, I had to stay away from my hometown. Though living in the city brings me a lot of convenience, and I eat the good food, I still miss country life all the time. I love to live in the country.\n",
    "\n",
    "Living in the country, the time seems to be very slow. I woke up early in the morning and then took the walk. After eating the breakfast, it was about 8 o’clock. I went out to play with my friends or went to help my grandparents with their work. After doing these, it was just 11 o’clock. But in the city, I woke up at 9 o’clock, and then I ate the breakfast, the rest of the day was to play computer. How time flies to me.\n",
    "\n",
    "I get so close to the nature and I find so much fun in the country life. I always want to live in my hometown. When I have the time, I will go back there and enjoy the moment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "142ab23f-296c-4f96-a59d-f5a664c0ae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' I was born in the country, my family lived there before I was 6 years old . My parents moved to the city for their work, I had to stay away from my hometown . Though living in the city brings me a lot of convenience, I still miss country life all the time .'}]\n"
     ]
    }
   ],
   "source": [
    "rs = pipe(text)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d930e-b0fc-4468-b424-7b35187f7037",
   "metadata": {},
   "source": [
    "## 指定 model 和 task 创建 pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b73645f-efbc-4118-ba45-e7f48064858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14b6f6bc-3f8f-457c-9767-786f9baefcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'I was born in the country, my family lived there before I was 6 years old . My parents moved to the city for their work, I had to stay away from my hometown . Living in the city brings me a lot of convenience, and I eat the good food .'}]\n"
     ]
    }
   ],
   "source": [
    "print(pipe(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2871a8d-af93-4bab-b98c-32c041348390",
   "metadata": {},
   "source": [
    "## 使用pre-trained model 创建pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ab986a4-3673-4f70-8a22-31cfc9c9ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('Falconsai/text_summarization')\n",
    "tokenizer = AutoTokenizer.from_pretrained('Falconsai/text_summarization')\n",
    "pipe = pipeline('summarization', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e3ba838-9409-4c4a-af11-10479a62c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'I was born in the country, my family lived there before I was 6 years old . My parents moved to the city for their work, I had to stay away from my hometown . Living in the city brings me a lot of convenience, and I eat the good food .'}]\n"
     ]
    }
   ],
   "source": [
    "print(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c92b4b55-3b55-4554-abf8-76d0008d4248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2009d52a-0b95-4a9d-a5b1-9766fdda508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8072702646255494\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "times = []\n",
    "for i in range(10):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    pipe(text)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "print(sum(times) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d23a3e5b-af48-4b78-8503-6a23701cd69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "操作耗时: 1845.3974609375 毫秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 同步，确保所有之前的CUDA操作都已完成\n",
    "torch.cuda.synchronize()\n",
    "# 开始时间\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "\n",
    "# 执行一些CUDA操作\n",
    "pipe(text)\n",
    "\n",
    "# 结束时间\n",
    "end.record()\n",
    "\n",
    "# 同步，确保所有CUDA操作都已完成\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算运行时间\n",
    "elapsed_time_ms = start.elapsed_time(end)\n",
    "print(f'操作耗时: {elapsed_time_ms} 毫秒')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c87b54-7abf-43a9-b1d5-9e800988a24f",
   "metadata": {},
   "source": [
    "## 上面的是使用cpu推理，下面使用gpu推理看看时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "507c7d1d-00a1-4a97-aae7-c03d5874dd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline('summarization', model=model, tokenizer=tokenizer, device=0)\n",
    "pipe.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef5b9b15-5dd4-4aca-801b-b27fd840824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6993857622146606\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "times = []\n",
    "for i in range(10):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    pipe(text)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "print(sum(times) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4182f712-cb95-4c5e-bd49-d3f1c92a9756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "操作耗时: 1003.6449584960938 毫秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 同步，确保所有之前的CUDA操作都已完成\n",
    "torch.cuda.synchronize()\n",
    "# 开始时间\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "\n",
    "# 执行一些CUDA操作\n",
    "pipe(text)\n",
    "\n",
    "# 结束时间\n",
    "end.record()\n",
    "\n",
    "# 同步，确保所有CUDA操作都已完成\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算运行时间\n",
    "elapsed_time_ms = start.elapsed_time(end)\n",
    "print(f'操作耗时: {elapsed_time_ms} 毫秒')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5bb2a-4d94-4ee3-9431-c22d0ceec2bf",
   "metadata": {},
   "source": [
    "## 试一下问答任务  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4679d2c5-be92-4841-ada9-9bdca6c1b41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc25953c0624778a59459203ef16ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuehu\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d205a9cf988f49a1b35301bb58d8d44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/407M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f4b0bfe7dc43d683fc8f0d5c7abd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8417ca8c5d7f4a51af0f496f023f98fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b17f9c900b74f0286e2bb4bc4699fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa = pipeline('question-answering', model=\"uer/roberta-base-chinese-extractive-qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5b0d7-7517-4f7b-9fae-bd12a425ae01",
   "metadata": {},
   "source": [
    "### 使用说明 \n",
    "1. `question`参数：这个参数用于提供用户想要提问的问题。在这个例子中，问题是“广东省省会是哪里？”。这个参数告诉模型需要寻找答案的问题内容。\n",
    "\n",
    "2. `context`参数：这个参数提供了一个文本段落，其中可能包含问题的答案。在这个例子中，上下文是“广东省省会是广州”。模型会在这个上下文中搜索并提取出问题的答案。\n",
    "\n",
    "`max_answer_len`参数是可选的，它指定了模型生成的答案的最大长度。在这个例子中，设置为2，意味着模型给出的答案不会超过2个字符。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e55f6457-bc09-4a7d-8c96-70afe7deda31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8817171454429626, 'start': 6, 'end': 8, 'answer': '广州'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"广东省省会是哪里？\", context=\"广东省省会是广州\", max_answer_len=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c554600d-dc59-47be-8abf-8583612b98fd",
   "metadata": {},
   "source": [
    "# pipeline实现原理和详细步骤\n",
    "Hugging Face Transformers库中的`pipeline`是一个高级API，它简化了常见NLP任务的实现过程。当你使用`pipeline`时，它在背后为你处理了很多细节，比如加载模型、处理输入数据、调用模型以及后处理输出结果。\n",
    "\n",
    "下面是`pipeline`背后的一般步骤和原理：\n",
    "1. **加载模型和 tokenizer**：\n",
    "   - `pipeline`首先根据你提供的任务名称（如`'question-answering'`）和模型名称（如`'uer/roberta-base-chinese-extractive-qa'`）加载相应的预训练模型和分词器（tokenizer）。\n",
    "   - 分词器用于将输入文本（问题和上下文）转换为模型可以理解的数字表示（即词嵌入）。\n",
    "2. **预处理输入数据**：\n",
    "   - `pipeline`使用分词器对输入的问题和上下文进行预处理，包括分词、转换为词嵌入、添加必要的特殊标记（如BERT的`[CLS]`和`[SEP]`）等。\n",
    "   - 预处理后的数据会被组织成模型期望的格式。\n",
    "3. **调用模型**：\n",
    "   - 预处理后的数据被送入加载的模型中。\n",
    "   - 模型根据输入数据计算输出，对于问答任务，这通常是一个表示答案开始和结束位置的分数分布。\n",
    "4. **后处理输出**：\n",
    "   - `pipeline`根据模型的输出进行后处理，例如，对于问答任务，它会从分数分布中找出最可能的答案开始和结束位置。\n",
    "   - 然后，它会从原始上下文中提取出对应的文本作为答案。\n",
    "5. **返回结果**：  \n",
    "   - 最后，`pipeline`返回处理后的结果，通常是问题的答案。\n",
    "\n",
    "在之前问答任务的上下文中，`pipeline`使用了一个名为“uer/roberta-base-chinese-extractive-qa”的模型，这是一个基于RoBERTa的模型，专门用于中文文本的提取式问答。这意味着它不是生成答案，而是从给定的上下文中提取答案。\n",
    "\n",
    "**`pipeline`的设计目的是为了简化常见NLP任务的实现，让用户可以不必深入了解模型和数据处理的具体细节，就能快速实现和部署模型。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51d50e01-10b2-466a-984f-c929ac0de27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4850a08-5ae8-46a8-b71e-baab7575a401",
   "metadata": {},
   "source": [
    "## step1 加载模型和tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417a58c-c5c4-4e97-8f72-17bd9711f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-chinese-extractive-qa\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"uer/roberta-base-chinese-extractive-qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99617ae-e10d-4b92-a860-19dbd4f187c1",
   "metadata": {},
   "source": [
    "## step2 预处理输入数据         \n",
    "貌似不加 `add_specical_tokens=True` 也没什么问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e489059-670f-4024-abb4-fc76924ee162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2408,  691, 4689, 4689,  833, 3221, 1525, 7027, 8043,  102, 2408,\n",
       "          691, 4689, 4689,  833, 3221, 2408, 2336,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, context = \"广东省省会是哪里？\", \"广东省省会是广州\"\n",
    "# inputs = tokenizer(question, context, return_tensors=\"pt\", add_special_tokens=True)\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b5b4a-4b05-4673-ab7c-02f82d3e3f27",
   "metadata": {},
   "source": [
    "## step3 调用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "17bc873f-d7b6-4ccc-8c44-0b90114002a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 2.8356, -2.5820, -6.5702, -8.1818, -8.0138, -8.1900, -6.5748, -4.0969,\n",
       "         -7.1589, -8.1996, -7.6051, -1.9516, -6.4490, -6.2248, -4.5364, -6.3773,\n",
       "         -4.8039,  5.6266, -0.9436, -7.5889]]), end_logits=tensor([[ 2.8049, -3.7779, -0.8281, -3.6546, -5.3831, -4.6677, -6.7706, -5.0973,\n",
       "         -3.9445, -8.3965, -8.5465, -5.2114, -2.4174, -3.6956, -5.1664, -4.2346,\n",
       "         -6.8528, -1.7553,  5.5331, -8.3205]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808f435-64ae-4f17-9dd4-0f15e1242bf9",
   "metadata": {},
   "source": [
    "## step4 后处理输出  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3ae3a2a-5225-4092-850f-db4df2f50581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_logits = outputs.start_logits\n",
    "# end_logits = outputs.end_logits\n",
    "# answer_start = torch.argmax(start_logits)\n",
    "# answer_end = torch.argmax(end_logits)\n",
    "# answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end+1]))\n",
    "# answer\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "answer_start = torch.argmax(start_logits)\n",
    "answer_end = torch.argmax(end_logits)\n",
    "\n",
    "# 确保结束位置在开始位置之后\n",
    "if answer_end < answer_start:\n",
    "    answer_end = answer_start + 2  # 至少包含2个字符 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b326c2f-8a8a-479f-860a-def5feb640d6",
   "metadata": {},
   "source": [
    "## step5 返回结果           \n",
    "两种方式都可以: \n",
    "- `convert_tokens_to_string(tokenizer.convert_ids_to_tokens())`\n",
    "\n",
    "- `tokenzier.decode()`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b6bd992-e5c8-4fbf-b991-5a5703021509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "广 州\n"
     ]
    }
   ],
   "source": [
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end+1]))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db954513-acf3-4f1e-ba06-98d2c8aa0d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'广 州'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_other = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end+1])\n",
    "answer_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b53db2a-3a18-4ab2-9d29-84a4842dd919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2408,  691, 4689, 4689,  833, 3221, 1525, 7027, 8043,  102, 2408,\n",
       "          691, 4689, 4689,  833, 3221, 2408, 2336,  102]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c3676e8-d3e3-4488-a4a2-ac7a746e7296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2408,  691, 4689, 4689,  833, 3221, 1525, 7027, 8043,  102, 2408,\n",
       "         691, 4689, 4689,  833, 3221, 2408, 2336,  102])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6ca368c-776c-4793-bcf4-49af0acc114a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2408, 2336])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][0][answer_start:answer_end+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e40f97-ce3d-4823-af07-c25ce0cef648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
