{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b5c41a-8d7f-4538-8936-1f17227eaaad",
   "metadata": {},
   "source": [
    "# 文本分类实战  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c47117-18d2-4d6b-9c16-d60a3bc71122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf418d5d-742e-47f0-b067-5f3aa81508b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\CodeLibrary\\\\NLP_Task\\\\classification_demo'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5210ae-f4f1-497b-b0d6-4da75370ff96",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c37527f-9fa3-4425-ae5d-a07a75c94dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='D:\\\\CodeLibrary\\\\NLP_Task\\\\classification_demo\\\\ChnSentiCorp_htl_all.csv', split='train')\n",
    "dataset = dataset.filter(lambda x : x['review'] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3a30bd-33b2-4752-aec6-6c768a04e200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较为简单.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1bc51c-a8fd-4b71-b2dd-75ce82b4edbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefb82a-acf1-4f6c-9ae3-0af354db53c6",
   "metadata": {},
   "source": [
    "## 划分数据\n",
    "```python\n",
    "def train_test_split(\n",
    "        self,\n",
    "        test_size: Union[float, int, None] = None,\n",
    "        train_size: Union[float, int, None] = None,\n",
    "        shuffle: bool = True,\n",
    "        stratify_by_column: Optional[str] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        generator: Optional[np.random.Generator] = None,\n",
    "        keep_in_memory: bool = False,\n",
    "        load_from_cache_file: Optional[bool] = None,\n",
    "        train_indices_cache_file_name: Optional[str] = None,\n",
    "        test_indices_cache_file_name: Optional[str] = None,\n",
    "        writer_batch_size: Optional[int] = 1000,\n",
    "        train_new_fingerprint: Optional[str] = None,\n",
    "        test_new_fingerprint: Optional[str] = None,\n",
    "    ) -> \"DatasetDict\":\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f0c801-83b5-4e38-b42e-9ec5d4b94313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = dataset.train_test_split(test_size=0.1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd02fed-0fe3-41fc-8a14-c088c6fa4b5d",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "1. `.map()`: 这是`datasets`库中的一个方法，用于对数据集进行变换。它会将一个函数应用到数据集的每个样本上。\n",
    "2. `batched=True`: 这个参数指示`.map()`方法以批处理的方式应用`process_datasets`函数。这意味着函数将被应用到一批数据上，而不是单个样本，这通常可以提高处理效率。\n",
    "3. `remove_columns=datasets['train'].column_names`: 这个参数指示`.map()`方法在处理完成后，从数据集中删除指定的列。这里的`datasets['train'].column_names`是一个包含训练集所有列名的列表。这通常用于删除在处理过程中不再需要的原始列，比如原始文本列，在文本被分词和转换为数字表示后。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6545effd-4645-4e06-a10b-a0b9684a36ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 6988\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ed81ba-c702-42da-98d7-23f3da5d9f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'review']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aafd3a3d-c446-4707-980a-8e407ff76db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-macbert-large')\n",
    "def process_datasets(examples):\n",
    "    tokenized_examples = tokenizer(examples['review'], max_length=32, truncation=True, padding=True)\n",
    "    tokenized_examples['label'] = examples['label'] # 重新添加 ['label']\n",
    "    return tokenized_examples\n",
    "\n",
    "# 带有 remove_columns 的 feature栏 少了 ['review'], 本质上 ['label'] 也被删除了，只是重新添加回去了\n",
    "tokenized_datasets = datasets.map(process_datasets, batched=True, remove_columns=datasets['train'].column_names)\n",
    "# tokenized_datasets = datasets.map(process_datasets, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d69b2-7dff-4f2e-96ee-6af0e6a6e00f",
   "metadata": {},
   "source": [
    "## 创建模型  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b424a5bc-8f98-482e-ac42-fec3ed451a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c21e7652b34290b5878ca5181a812d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuehu\\.cache\\huggingface\\hub\\models--hfl--chinese-macbert-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-macbert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d820a74-a4a6-4f69-95a6-649bced114d7",
   "metadata": {},
   "source": [
    "## 创建评估函数  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "215cedaf-2a0b-413a-8f13-9cf3f9e44ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load('D:\\\\CodeLibrary\\\\NLP_Task\\\\classification_demo\\\\metric_accuracy.py')\n",
    "f1_metirc = evaluate.load('D:\\\\CodeLibrary\\\\NLP_Task\\\\classification_demo\\\\metric_f1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "962ad5c2-2f73-45b0-89f9-5f2a0933cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(eval_predict):\n",
    "    predictions, labels = eval_predict\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metirc.compute(predictions=predictions, references=labels)\n",
    "    acc.update(f1)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a9dcd-9541-46b4-990b-0f802e70540d",
   "metadata": {},
   "source": [
    "## TrainingArguments      \n",
    "\n",
    "- `gradient_aacumulation_steps` 就是为了模拟多 batch训练而已，只更新一次梯度，这样能减少显存占用的同时又用多batch训练\n",
    "\n",
    "\n",
    "- `optim` 可能是用 'adafactor' 占显存比较少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "500026d2-c4e6-4312-9ed5-cb746100728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(output_dir=\"./checkpoints\",      # 输出文件夹\n",
    "                               per_device_train_batch_size=1,   # 训练时的batch_size\n",
    "                               gradient_accumulation_steps=32,  # *** 梯度累加 ***\n",
    "                               gradient_checkpointing=True,     # *** 梯度检查点 ***\n",
    "                               optim=\"adafactor\",               # *** adafactor优化器 *** \n",
    "                               per_device_eval_batch_size=1,    # 验证时的batch_size\n",
    "                               num_train_epochs=1,              # 训练轮数\n",
    "                               logging_steps=10,                # log 打印的频率\n",
    "                               eval_strategy=\"epoch\",     # 评估策略\n",
    "                               save_strategy=\"epoch\",           # 保存策略\n",
    "                               save_total_limit=3,              # 最大保存数\n",
    "                               learning_rate=2e-5,              # 学习率\n",
    "                               weight_decay=0.01,               # weight_decay\n",
    "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               load_best_model_at_end=True)     # 训练完成后加载最优模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c0500-61fd-403a-b41c-ea782d46f1fe",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3fba960-eb7f-48b7-8a59-50a90775df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# *** 参数冻结 *** \n",
    "for name, param in model.bert.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                  args=train_args, \n",
    "                  train_dataset=tokenized_datasets[\"train\"], \n",
    "                  eval_dataset=tokenized_datasets[\"test\"], \n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "                  compute_metrics=eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff9930-8cc3-4243-91e7-e60e2d6990da",
   "metadata": {},
   "source": [
    "## model training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b399ddc4-bae5-431d-83ae-d24384f81a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [218/218 03:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.606409</td>\n",
       "      <td>0.694981</td>\n",
       "      <td>0.820046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=218, training_loss=0.6196840487488913, metrics={'train_runtime': 185.1208, 'train_samples_per_second': 37.748, 'train_steps_per_second': 1.178, 'total_flos': 406322074411008.0, 'train_loss': 0.6196840487488913, 'epoch': 0.998282770463652})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "936d049c-da3d-48c8-8344-513ebfff72ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='777' max='777' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [777/777 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6064087748527527,\n",
       " 'eval_accuracy': 0.694980694980695,\n",
       " 'eval_f1': 0.8200455580865603,\n",
       " 'eval_runtime': 11.1152,\n",
       " 'eval_samples_per_second': 69.905,\n",
       " 'eval_steps_per_second': 69.905,\n",
       " 'epoch': 0.998282770463652}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688671e2-99b8-4c31-b47f-7e8abfd30fcc",
   "metadata": {},
   "source": [
    "## model predictions     \n",
    "\n",
    "这训练结果属实难崩，毕竟显存只用了2G。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c16f5d3-de20-4c74-8304-e7316d0c5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入：杭州酒家的菜很难吃!\n",
      "模型预测结果:好评！\n"
     ]
    }
   ],
   "source": [
    "sen = \"杭州酒家的菜很难吃!\"\n",
    "id2_label = {0: \"差评！\", 1: \"好评！\"}\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    logits = model(**inputs).logits\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    print(f\"输入：{sen}\\n模型预测结果:{id2_label.get(pred.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09b93c50-80cd-455a-9a0c-c6cb430195ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model.config.id2label = id2_label\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fc4d76e-cc31-42ca-8720-9f2cc57e8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '好评！', 'score': 0.7304642796516418}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2609d24-9761-4598-879b-cfde5ebcf243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
