{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b0916a-b07e-48fd-9f29-60bcc57bddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e84e1c0-9b8c-4def-b964-33ffd896495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def load(\n",
      "    path: str,\n",
      "    config_name: Optional[str] = None,\n",
      "    module_type: Optional[str] = None,\n",
      "    process_id: int = 0,\n",
      "    num_process: int = 1,\n",
      "    cache_dir: Optional[str] = None,\n",
      "    experiment_id: Optional[str] = None,\n",
      "    keep_in_memory: bool = False,\n",
      "    download_config: Optional[DownloadConfig] = None,\n",
      "    download_mode: Optional[DownloadMode] = None,\n",
      "    revision: Optional[Union[str, Version]] = None,\n",
      "    **init_kwargs,\n",
      ") -> EvaluationModule:\n",
      "    \"\"\"Load a [`~evaluate.EvaluationModule`].\n",
      "\n",
      "    Args:\n",
      "\n",
      "        path (`str`):\n",
      "            Path to the evaluation processing script with the evaluation builder. Can be either:\n",
      "                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n",
      "                    e.g. `'./metrics/rouge'` or `'./metrics/rouge/rouge.py'`\n",
      "                - a evaluation module identifier on the HuggingFace evaluate repo e.g. `'rouge'` or `'bleu'` that are in either `'metrics/'`,\n",
      "                    `'comparisons/'`, or `'measurements/'` depending on the provided `module_type`\n",
      "        config_name (`str`, *optional*):\n",
      "            Selecting a configuration for the metric (e.g. the GLUE metric has a configuration for each subset).\n",
      "        module_type (`str`, default `'metric'`):\n",
      "            Type of evaluation module, can be one of `'metric'`, `'comparison'`, or `'measurement'`.\n",
      "        process_id (`int`, *optional*):\n",
      "            For distributed evaluation: id of the process.\n",
      "        num_process (`int`, *optional*):\n",
      "            For distributed evaluation: total number of processes.\n",
      "        cache_dir (`str`, *optional*):\n",
      "            Path to store the temporary predictions and references (default to `~/.cache/huggingface/evaluate/`).\n",
      "        experiment_id (`str`):\n",
      "            A specific experiment id. This is used if several distributed evaluations share the same file system.\n",
      "            This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n",
      "        keep_in_memory (`bool`):\n",
      "            Whether to store the temporary results in memory (defaults to `False`).\n",
      "        download_config ([`~evaluate.DownloadConfig`], *optional*):\n",
      "            Specific download configuration parameters.\n",
      "        download_mode ([`DownloadMode`], defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "            Download/generate mode.\n",
      "        revision (`Union[str, evaluate.Version]`, *optional*):\n",
      "            If specified, the module will be loaded from the datasets repository\n",
      "            at this version. By default it is set to the local version of the lib. Specifying a version that is different from\n",
      "            your local version of the lib might cause compatibility issues.\n",
      "\n",
      "    Returns:\n",
      "        [`evaluate.EvaluationModule`]\n",
      "\n",
      "    Example:\n",
      "\n",
      "        ```py\n",
      "        >>> from evaluate import load\n",
      "        >>> accuracy = load(\"accuracy\")\n",
      "        ```\n",
      "    \"\"\"\n",
      "    download_mode = DownloadMode(download_mode or DownloadMode.REUSE_DATASET_IF_EXISTS)\n",
      "    evaluation_module = evaluation_module_factory(\n",
      "        path, module_type=module_type, revision=revision, download_config=download_config, download_mode=download_mode\n",
      "    )\n",
      "    evaluation_cls = import_main_class(evaluation_module.module_path)\n",
      "    evaluation_instance = evaluation_cls(\n",
      "        config_name=config_name,\n",
      "        process_id=process_id,\n",
      "        num_process=num_process,\n",
      "        cache_dir=cache_dir,\n",
      "        keep_in_memory=keep_in_memory,\n",
      "        experiment_id=experiment_id,\n",
      "        hash=evaluation_module.hash,\n",
      "        **init_kwargs,\n",
      "    )\n",
      "\n",
      "    if module_type and module_type != evaluation_instance.module_type:\n",
      "        raise TypeError(\n",
      "            f\"No module of module type '{module_type}' not found for '{path}' locally, or on the Hugging Face Hub. Found module of module type '{evaluation_instance.module_type}' instead.\"\n",
      "        )\n",
      "\n",
      "    # Download and prepare resources for the metric\n",
      "    evaluation_instance.download_and_prepare(download_config=download_config)\n",
      "\n",
      "    return evaluation_instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rs = inspect.getsource(evaluate.load)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96178c6-bfeb-443e-be79-77a3bdd59db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import inspect\n",
    "\n",
    "print(inspect.getsource(transformers.pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3038b65-749e-42f5-8d33-fea818611610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05e91e1-bc6e-4155-9cc0-73e18c69086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c1f4aa7a7f43108ee93dd97e6f7da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuehu\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b40d2cf1f4248b78bf94fc01c691a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67ac7646ef14bd6a6c30c30aedbc198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f01ff496d544454a1991346305fc50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e824b600024427aa082e1f62338ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23b3e05b-eea3-4247-8ffb-ab5eec6d076d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' 99% of accuracy is based on accuracy, accuracy and accuracy . Accuracy is 99%, accuracy is 99%, accuracy is 100% . Accuracy \\xa0is 99% and accuracy is only 99% per cent, accuracy rate is 1.5%, accuracy rate of 1,000 per cent .'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Accuracy是衡量分类模型的最直白的指标，但缺陷也是明显的。假设有100个样本，其中有99个都是正样本，则分类器只需要一直预测为正例，就可以得到99%的准确率，实际上这个分类器性能是很低下的。也就是说，当不同类别的样本所占的比例严重不平衡时，占比大的类别会是影响准确率的最主要的因素。所以，只有当数据集各个类别的样本比例比较均衡时，Accuracy这个指标才是一个比较好的衡量标准。因此，必须参考其他指标才能完整评估模型的性能。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5205f2-b911-47ea-9bbe-18e1e05211b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text2text_generation.SummarizationPipeline object at 0x000001AC41BFDA10>\n"
     ]
    }
   ],
   "source": [
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d371a8-adbb-4325-8da8-3ddef633556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_batch_size', '_create_repo', '_ensure_tensor_on_device', '_forward', '_forward_params', '_get_files_timestamps', '_num_workers', '_parse_and_tokenize', '_postprocess_params', '_preprocess_params', '_sanitize_parameters', '_upload_modified_files', 'binary_output', 'call_count', 'check_inputs', 'check_model_type', 'default_input_names', 'device', 'device_placement', 'ensure_tensor_on_device', 'feature_extractor', 'forward', 'framework', 'get_inference_context', 'get_iterator', 'image_processor', 'iterate', 'model', 'modelcard', 'postprocess', 'predict', 'preprocess', 'push_to_hub', 'return_name', 'run_multi', 'run_single', 'save_pretrained', 'task', 'tokenizer', 'torch_dtype', 'transform']\n"
     ]
    }
   ],
   "source": [
    "rs = dir(pipe)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8a5f366-3841-44fb-8d97-ac0d3b79a6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Pipeline.check_model_type of <transformers.pipelines.text2text_generation.SummarizationPipeline object at 0x000001AC41BFDA10>>\n"
     ]
    }
   ],
   "source": [
    "print(pipe.check_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e50d93f0-33d5-49e1-9fcf-c97a544dd30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def pipeline(\n",
      "    task: str = None,\n",
      "    model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None,\n",
      "    config: Optional[Union[str, PretrainedConfig]] = None,\n",
      "    tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None,\n",
      "    feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None,\n",
      "    image_processor: Optional[Union[str, BaseImageProcessor]] = None,\n",
      "    framework: Optional[str] = None,\n",
      "    revision: Optional[str] = None,\n",
      "    use_fast: bool = True,\n",
      "    token: Optional[Union[str, bool]] = None,\n",
      "    device: Optional[Union[int, str, \"torch.device\"]] = None,\n",
      "    device_map=None,\n",
      "    torch_dtype=None,\n",
      "    trust_remote_code: Optional[bool] = None,\n",
      "    model_kwargs: Dict[str, Any] = None,\n",
      "    pipeline_class: Optional[Any] = None,\n",
      "    **kwargs,\n",
      ") -> Pipeline:\n",
      "    \"\"\"\n",
      "    Utility factory method to build a [`Pipeline`].\n",
      "\n",
      "    Pipelines are made of:\n",
      "\n",
      "        - A [tokenizer](tokenizer) in charge of mapping raw textual input to token.\n",
      "        - A [model](model) to make predictions from the inputs.\n",
      "        - Some (optional) post processing for enhancing model's output.\n",
      "\n",
      "    Args:\n",
      "        task (`str`):\n",
      "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
      "\n",
      "            - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
      "            - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
      "            - `\"conversational\"`: will return a [`ConversationalPipeline`].\n",
      "            - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
      "            - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
      "            - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
      "            - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
      "            - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
      "            - `\"image-feature-extraction\"`: will return an [`ImageFeatureExtractionPipeline`].\n",
      "            - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
      "            - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n",
      "            - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
      "            - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
      "            - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
      "            - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
      "            - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
      "            - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
      "            - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
      "            - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
      "              [`TextClassificationPipeline`].\n",
      "            - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
      "            - `\"text-to-audio\"` (alias `\"text-to-speech\"` available): will return a [`TextToAudioPipeline`]:.\n",
      "            - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
      "            - `\"translation\"`: will return a [`TranslationPipeline`].\n",
      "            - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
      "            - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
      "            - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
      "            - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
      "            - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
      "            - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
      "            - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`].\n",
      "\n",
      "        model (`str` or [`PreTrainedModel`] or [`TFPreTrainedModel`], *optional*):\n",
      "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
      "            actual instance of a pretrained model inheriting from [`PreTrainedModel`] (for PyTorch) or\n",
      "            [`TFPreTrainedModel`] (for TensorFlow).\n",
      "\n",
      "            If not provided, the default for the `task` will be loaded.\n",
      "        config (`str` or [`PretrainedConfig`], *optional*):\n",
      "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
      "            identifier or an actual pretrained model configuration inheriting from [`PretrainedConfig`].\n",
      "\n",
      "            If not provided, the default configuration file for the requested model will be used. That means that if\n",
      "            `model` is given, its default configuration will be used. However, if `model` is not supplied, this\n",
      "            `task`'s default model's config is used instead.\n",
      "        tokenizer (`str` or [`PreTrainedTokenizer`], *optional*):\n",
      "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained tokenizer inheriting from [`PreTrainedTokenizer`].\n",
      "\n",
      "            If not provided, the default tokenizer for the given `model` will be loaded (if it is a string). If `model`\n",
      "            is not specified or not a string, then the default tokenizer for `config` is loaded (if it is a string).\n",
      "            However, if `config` is also not given or not a string, then the default tokenizer for the given `task`\n",
      "            will be loaded.\n",
      "        feature_extractor (`str` or [`PreTrainedFeatureExtractor`], *optional*):\n",
      "            The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained feature extractor inheriting from [`PreTrainedFeatureExtractor`].\n",
      "\n",
      "            Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
      "            models. Multi-modal models will also require a tokenizer to be passed.\n",
      "\n",
      "            If not provided, the default feature extractor for the given `model` will be loaded (if it is a string). If\n",
      "            `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n",
      "            is a string). However, if `config` is also not given or not a string, then the default feature extractor\n",
      "            for the given `task` will be loaded.\n",
      "        framework (`str`, *optional*):\n",
      "            The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "            installed.\n",
      "\n",
      "            If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "            provided.\n",
      "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "            When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
      "            branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
      "            artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n",
      "        use_fast (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n",
      "        use_auth_token (`str` or *bool*, *optional*):\n",
      "            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      "            when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      "        device (`int` or `str` or `torch.device`):\n",
      "            Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n",
      "            pipeline will be allocated.\n",
      "        device_map (`str` or `Dict[str, Union[int, str, torch.device]`, *optional*):\n",
      "            Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate` library is present, set\n",
      "            `device_map=\"auto\"` to compute the most optimized `device_map` automatically (see\n",
      "            [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)\n",
      "            for more information).\n",
      "\n",
      "            <Tip warning={true}>\n",
      "\n",
      "            Do not use `device_map` AND `device` at the same time as they will conflict\n",
      "\n",
      "            </Tip>\n",
      "\n",
      "        torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      "            Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      "            (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`).\n",
      "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to allow for custom code defined on the Hub in their own modeling, configuration,\n",
      "            tokenization or even pipeline files. This option should only be set to `True` for repositories you trust\n",
      "            and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
      "        model_kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\n",
      "            **model_kwargs)` function.\n",
      "        kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
      "            corresponding pipeline class for possible values).\n",
      "\n",
      "    Returns:\n",
      "        [`Pipeline`]: A suitable pipeline for the task.\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    ```python\n",
      "    >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
      "\n",
      "    >>> # Sentiment analysis pipeline\n",
      "    >>> analyzer = pipeline(\"sentiment-analysis\")\n",
      "\n",
      "    >>> # Question answering pipeline, specifying the checkpoint identifier\n",
      "    >>> oracle = pipeline(\n",
      "    ...     \"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", tokenizer=\"google-bert/bert-base-cased\"\n",
      "    ... )\n",
      "\n",
      "    >>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
      "    >>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
      "    >>> recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
      "    ```\"\"\"\n",
      "    if model_kwargs is None:\n",
      "        model_kwargs = {}\n",
      "    # Make sure we only pass use_auth_token once as a kwarg (it used to be possible to pass it in model_kwargs,\n",
      "    # this is to keep BC).\n",
      "    use_auth_token = model_kwargs.pop(\"use_auth_token\", None)\n",
      "    if use_auth_token is not None:\n",
      "        warnings.warn(\n",
      "            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
      "            FutureWarning,\n",
      "        )\n",
      "        if token is not None:\n",
      "            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n",
      "        token = use_auth_token\n",
      "\n",
      "    code_revision = kwargs.pop(\"code_revision\", None)\n",
      "    commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
      "\n",
      "    hub_kwargs = {\n",
      "        \"revision\": revision,\n",
      "        \"token\": token,\n",
      "        \"trust_remote_code\": trust_remote_code,\n",
      "        \"_commit_hash\": commit_hash,\n",
      "    }\n",
      "\n",
      "    if task is None and model is None:\n",
      "        raise RuntimeError(\n",
      "            \"Impossible to instantiate a pipeline without either a task or a model \"\n",
      "            \"being specified. \"\n",
      "            \"Please provide a task class or a model\"\n",
      "        )\n",
      "\n",
      "    if model is None and tokenizer is not None:\n",
      "        raise RuntimeError(\n",
      "            \"Impossible to instantiate a pipeline with tokenizer specified but not the model as the provided tokenizer\"\n",
      "            \" may not be compatible with the default model. Please provide a PreTrainedModel class or a\"\n",
      "            \" path/identifier to a pretrained model when providing tokenizer.\"\n",
      "        )\n",
      "    if model is None and feature_extractor is not None:\n",
      "        raise RuntimeError(\n",
      "            \"Impossible to instantiate a pipeline with feature_extractor specified but not the model as the provided\"\n",
      "            \" feature_extractor may not be compatible with the default model. Please provide a PreTrainedModel class\"\n",
      "            \" or a path/identifier to a pretrained model when providing feature_extractor.\"\n",
      "        )\n",
      "    if isinstance(model, Path):\n",
      "        model = str(model)\n",
      "\n",
      "    if commit_hash is None:\n",
      "        pretrained_model_name_or_path = None\n",
      "        if isinstance(config, str):\n",
      "            pretrained_model_name_or_path = config\n",
      "        elif config is None and isinstance(model, str):\n",
      "            pretrained_model_name_or_path = model\n",
      "\n",
      "        if not isinstance(config, PretrainedConfig) and pretrained_model_name_or_path is not None:\n",
      "            # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n",
      "            resolved_config_file = cached_file(\n",
      "                pretrained_model_name_or_path,\n",
      "                CONFIG_NAME,\n",
      "                _raise_exceptions_for_gated_repo=False,\n",
      "                _raise_exceptions_for_missing_entries=False,\n",
      "                _raise_exceptions_for_connection_errors=False,\n",
      "                cache_dir=model_kwargs.get(\"cache_dir\"),\n",
      "                **hub_kwargs,\n",
      "            )\n",
      "            hub_kwargs[\"_commit_hash\"] = extract_commit_hash(resolved_config_file, commit_hash)\n",
      "        else:\n",
      "            hub_kwargs[\"_commit_hash\"] = getattr(config, \"_commit_hash\", None)\n",
      "\n",
      "    # Config is the primordial information item.\n",
      "    # Instantiate config if needed\n",
      "    if isinstance(config, str):\n",
      "        config = AutoConfig.from_pretrained(\n",
      "            config, _from_pipeline=task, code_revision=code_revision, **hub_kwargs, **model_kwargs\n",
      "        )\n",
      "        hub_kwargs[\"_commit_hash\"] = config._commit_hash\n",
      "    elif config is None and isinstance(model, str):\n",
      "        # Check for an adapter file in the model path if PEFT is available\n",
      "        if is_peft_available():\n",
      "            # `find_adapter_config_file` doesn't accept `trust_remote_code`\n",
      "            _hub_kwargs = {k: v for k, v in hub_kwargs.items() if k != \"trust_remote_code\"}\n",
      "            maybe_adapter_path = find_adapter_config_file(\n",
      "                model,\n",
      "                token=hub_kwargs[\"token\"],\n",
      "                revision=hub_kwargs[\"revision\"],\n",
      "                _commit_hash=hub_kwargs[\"_commit_hash\"],\n",
      "            )\n",
      "\n",
      "            if maybe_adapter_path is not None:\n",
      "                with open(maybe_adapter_path, \"r\", encoding=\"utf-8\") as f:\n",
      "                    adapter_config = json.load(f)\n",
      "                    model = adapter_config[\"base_model_name_or_path\"]\n",
      "\n",
      "        config = AutoConfig.from_pretrained(\n",
      "            model, _from_pipeline=task, code_revision=code_revision, **hub_kwargs, **model_kwargs\n",
      "        )\n",
      "        hub_kwargs[\"_commit_hash\"] = config._commit_hash\n",
      "\n",
      "    custom_tasks = {}\n",
      "    if config is not None and len(getattr(config, \"custom_pipelines\", {})) > 0:\n",
      "        custom_tasks = config.custom_pipelines\n",
      "        if task is None and trust_remote_code is not False:\n",
      "            if len(custom_tasks) == 1:\n",
      "                task = list(custom_tasks.keys())[0]\n",
      "            else:\n",
      "                raise RuntimeError(\n",
      "                    \"We can't infer the task automatically for this model as there are multiple tasks available. Pick \"\n",
      "                    f\"one in {', '.join(custom_tasks.keys())}\"\n",
      "                )\n",
      "\n",
      "    if task is None and model is not None:\n",
      "        if not isinstance(model, str):\n",
      "            raise RuntimeError(\n",
      "                \"Inferring the task automatically requires to check the hub with a model_id defined as a `str`. \"\n",
      "                f\"{model} is not a valid model_id.\"\n",
      "            )\n",
      "        task = get_task(model, token)\n",
      "\n",
      "    # Retrieve the task\n",
      "    if task in custom_tasks:\n",
      "        normalized_task = task\n",
      "        targeted_task, task_options = clean_custom_task(custom_tasks[task])\n",
      "        if pipeline_class is None:\n",
      "            if not trust_remote_code:\n",
      "                raise ValueError(\n",
      "                    \"Loading this pipeline requires you to execute the code in the pipeline file in that\"\n",
      "                    \" repo on your local machine. Make sure you have read the code there to avoid malicious use, then\"\n",
      "                    \" set the option `trust_remote_code=True` to remove this error.\"\n",
      "                )\n",
      "            class_ref = targeted_task[\"impl\"]\n",
      "            pipeline_class = get_class_from_dynamic_module(\n",
      "                class_ref,\n",
      "                model,\n",
      "                code_revision=code_revision,\n",
      "                **hub_kwargs,\n",
      "            )\n",
      "    else:\n",
      "        normalized_task, targeted_task, task_options = check_task(task)\n",
      "        if pipeline_class is None:\n",
      "            pipeline_class = targeted_task[\"impl\"]\n",
      "\n",
      "    # Use default model/config/tokenizer for the task if no model is provided\n",
      "    if model is None:\n",
      "        # At that point framework might still be undetermined\n",
      "        model, default_revision = get_default_model_and_revision(targeted_task, framework, task_options)\n",
      "        revision = revision if revision is not None else default_revision\n",
      "        logger.warning(\n",
      "            f\"No model was supplied, defaulted to {model} and revision\"\n",
      "            f\" {revision} ({HUGGINGFACE_CO_RESOLVE_ENDPOINT}/{model}).\\n\"\n",
      "            \"Using a pipeline without specifying a model name and revision in production is not recommended.\"\n",
      "        )\n",
      "        if config is None and isinstance(model, str):\n",
      "            config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs)\n",
      "            hub_kwargs[\"_commit_hash\"] = config._commit_hash\n",
      "\n",
      "    if device_map is not None:\n",
      "        if \"device_map\" in model_kwargs:\n",
      "            raise ValueError(\n",
      "                'You cannot use both `pipeline(... device_map=..., model_kwargs={\"device_map\":...})` as those'\n",
      "                \" arguments might conflict, use only one.)\"\n",
      "            )\n",
      "        if device is not None:\n",
      "            logger.warning(\n",
      "                \"Both `device` and `device_map` are specified. `device` will override `device_map`. You\"\n",
      "                \" will most likely encounter unexpected behavior. Please remove `device` and keep `device_map`.\"\n",
      "            )\n",
      "        model_kwargs[\"device_map\"] = device_map\n",
      "    if torch_dtype is not None:\n",
      "        if \"torch_dtype\" in model_kwargs:\n",
      "            raise ValueError(\n",
      "                'You cannot use both `pipeline(... torch_dtype=..., model_kwargs={\"torch_dtype\":...})` as those'\n",
      "                \" arguments might conflict, use only one.)\"\n",
      "            )\n",
      "        if isinstance(torch_dtype, str) and hasattr(torch, torch_dtype):\n",
      "            torch_dtype = getattr(torch, torch_dtype)\n",
      "        model_kwargs[\"torch_dtype\"] = torch_dtype\n",
      "\n",
      "    model_name = model if isinstance(model, str) else None\n",
      "\n",
      "    # Load the correct model if possible\n",
      "    # Infer the framework from the model if not already defined\n",
      "    if isinstance(model, str) or framework is None:\n",
      "        model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}\n",
      "        framework, model = infer_framework_load_model(\n",
      "            model,\n",
      "            model_classes=model_classes,\n",
      "            config=config,\n",
      "            framework=framework,\n",
      "            task=task,\n",
      "            **hub_kwargs,\n",
      "            **model_kwargs,\n",
      "        )\n",
      "\n",
      "    model_config = model.config\n",
      "    hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\n",
      "    load_tokenizer = type(model_config) in TOKENIZER_MAPPING or model_config.tokenizer_class is not None\n",
      "    load_feature_extractor = type(model_config) in FEATURE_EXTRACTOR_MAPPING or feature_extractor is not None\n",
      "    load_image_processor = type(model_config) in IMAGE_PROCESSOR_MAPPING or image_processor is not None\n",
      "\n",
      "    # If `model` (instance of `PretrainedModel` instead of `str`) is passed (and/or same for config), while\n",
      "    # `image_processor` or `feature_extractor` is `None`, the loading will fail. This happens particularly for some\n",
      "    # vision tasks when calling `pipeline()` with `model` and only one of the `image_processor` and `feature_extractor`.\n",
      "    # TODO: we need to make `NO_IMAGE_PROCESSOR_TASKS` and `NO_FEATURE_EXTRACTOR_TASKS` more robust to avoid such issue.\n",
      "    # This block is only temporarily to make CI green.\n",
      "    if load_image_processor and load_feature_extractor:\n",
      "        load_feature_extractor = False\n",
      "\n",
      "    if (\n",
      "        tokenizer is None\n",
      "        and not load_tokenizer\n",
      "        and normalized_task not in NO_TOKENIZER_TASKS\n",
      "        # Using class name to avoid importing the real class.\n",
      "        and (\n",
      "            model_config.__class__.__name__ in MULTI_MODEL_AUDIO_CONFIGS\n",
      "            or model_config.__class__.__name__ in MULTI_MODEL_VISION_CONFIGS\n",
      "        )\n",
      "    ):\n",
      "        # This is a special category of models, that are fusions of multiple models\n",
      "        # so the model_config might not define a tokenizer, but it seems to be\n",
      "        # necessary for the task, so we're force-trying to load it.\n",
      "        load_tokenizer = True\n",
      "    if (\n",
      "        image_processor is None\n",
      "        and not load_image_processor\n",
      "        and normalized_task not in NO_IMAGE_PROCESSOR_TASKS\n",
      "        # Using class name to avoid importing the real class.\n",
      "        and model_config.__class__.__name__ in MULTI_MODEL_VISION_CONFIGS\n",
      "    ):\n",
      "        # This is a special category of models, that are fusions of multiple models\n",
      "        # so the model_config might not define a tokenizer, but it seems to be\n",
      "        # necessary for the task, so we're force-trying to load it.\n",
      "        load_image_processor = True\n",
      "    if (\n",
      "        feature_extractor is None\n",
      "        and not load_feature_extractor\n",
      "        and normalized_task not in NO_FEATURE_EXTRACTOR_TASKS\n",
      "        # Using class name to avoid importing the real class.\n",
      "        and model_config.__class__.__name__ in MULTI_MODEL_AUDIO_CONFIGS\n",
      "    ):\n",
      "        # This is a special category of models, that are fusions of multiple models\n",
      "        # so the model_config might not define a tokenizer, but it seems to be\n",
      "        # necessary for the task, so we're force-trying to load it.\n",
      "        load_feature_extractor = True\n",
      "\n",
      "    if task in NO_TOKENIZER_TASKS:\n",
      "        # These will never require a tokenizer.\n",
      "        # the model on the other hand might have a tokenizer, but\n",
      "        # the files could be missing from the hub, instead of failing\n",
      "        # on such repos, we just force to not load it.\n",
      "        load_tokenizer = False\n",
      "\n",
      "    if task in NO_FEATURE_EXTRACTOR_TASKS:\n",
      "        load_feature_extractor = False\n",
      "    if task in NO_IMAGE_PROCESSOR_TASKS:\n",
      "        load_image_processor = False\n",
      "\n",
      "    if load_tokenizer:\n",
      "        # Try to infer tokenizer from model or config name (if provided as str)\n",
      "        if tokenizer is None:\n",
      "            if isinstance(model_name, str):\n",
      "                tokenizer = model_name\n",
      "            elif isinstance(config, str):\n",
      "                tokenizer = config\n",
      "            else:\n",
      "                # Impossible to guess what is the right tokenizer here\n",
      "                raise Exception(\n",
      "                    \"Impossible to guess which tokenizer to use. \"\n",
      "                    \"Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
      "                )\n",
      "\n",
      "        # Instantiate tokenizer if needed\n",
      "        if isinstance(tokenizer, (str, tuple)):\n",
      "            if isinstance(tokenizer, tuple):\n",
      "                # For tuple we have (tokenizer name, {kwargs})\n",
      "                use_fast = tokenizer[1].pop(\"use_fast\", use_fast)\n",
      "                tokenizer_identifier = tokenizer[0]\n",
      "                tokenizer_kwargs = tokenizer[1]\n",
      "            else:\n",
      "                tokenizer_identifier = tokenizer\n",
      "                tokenizer_kwargs = model_kwargs.copy()\n",
      "                tokenizer_kwargs.pop(\"torch_dtype\", None)\n",
      "\n",
      "            tokenizer = AutoTokenizer.from_pretrained(\n",
      "                tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs\n",
      "            )\n",
      "\n",
      "    if load_image_processor:\n",
      "        # Try to infer image processor from model or config name (if provided as str)\n",
      "        if image_processor is None:\n",
      "            if isinstance(model_name, str):\n",
      "                image_processor = model_name\n",
      "            elif isinstance(config, str):\n",
      "                image_processor = config\n",
      "            # Backward compatibility, as `feature_extractor` used to be the name\n",
      "            # for `ImageProcessor`.\n",
      "            elif feature_extractor is not None and isinstance(feature_extractor, BaseImageProcessor):\n",
      "                image_processor = feature_extractor\n",
      "            else:\n",
      "                # Impossible to guess what is the right image_processor here\n",
      "                raise Exception(\n",
      "                    \"Impossible to guess which image processor to use. \"\n",
      "                    \"Please provide a PreTrainedImageProcessor class or a path/identifier \"\n",
      "                    \"to a pretrained image processor.\"\n",
      "                )\n",
      "\n",
      "        # Instantiate image_processor if needed\n",
      "        if isinstance(image_processor, (str, tuple)):\n",
      "            image_processor = AutoImageProcessor.from_pretrained(\n",
      "                image_processor, _from_pipeline=task, **hub_kwargs, **model_kwargs\n",
      "            )\n",
      "\n",
      "    if load_feature_extractor:\n",
      "        # Try to infer feature extractor from model or config name (if provided as str)\n",
      "        if feature_extractor is None:\n",
      "            if isinstance(model_name, str):\n",
      "                feature_extractor = model_name\n",
      "            elif isinstance(config, str):\n",
      "                feature_extractor = config\n",
      "            else:\n",
      "                # Impossible to guess what is the right feature_extractor here\n",
      "                raise Exception(\n",
      "                    \"Impossible to guess which feature extractor to use. \"\n",
      "                    \"Please provide a PreTrainedFeatureExtractor class or a path/identifier \"\n",
      "                    \"to a pretrained feature extractor.\"\n",
      "                )\n",
      "\n",
      "        # Instantiate feature_extractor if needed\n",
      "        if isinstance(feature_extractor, (str, tuple)):\n",
      "            feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
      "                feature_extractor, _from_pipeline=task, **hub_kwargs, **model_kwargs\n",
      "            )\n",
      "\n",
      "            if (\n",
      "                feature_extractor._processor_class\n",
      "                and feature_extractor._processor_class.endswith(\"WithLM\")\n",
      "                and isinstance(model_name, str)\n",
      "            ):\n",
      "                try:\n",
      "                    import kenlm  # to trigger `ImportError` if not installed\n",
      "                    from pyctcdecode import BeamSearchDecoderCTC\n",
      "\n",
      "                    if os.path.isdir(model_name) or os.path.isfile(model_name):\n",
      "                        decoder = BeamSearchDecoderCTC.load_from_dir(model_name)\n",
      "                    else:\n",
      "                        language_model_glob = os.path.join(\n",
      "                            BeamSearchDecoderCTC._LANGUAGE_MODEL_SERIALIZED_DIRECTORY, \"*\"\n",
      "                        )\n",
      "                        alphabet_filename = BeamSearchDecoderCTC._ALPHABET_SERIALIZED_FILENAME\n",
      "                        allow_patterns = [language_model_glob, alphabet_filename]\n",
      "                        decoder = BeamSearchDecoderCTC.load_from_hf_hub(model_name, allow_patterns=allow_patterns)\n",
      "\n",
      "                    kwargs[\"decoder\"] = decoder\n",
      "                except ImportError as e:\n",
      "                    logger.warning(f\"Could not load the `decoder` for {model_name}. Defaulting to raw CTC. Error: {e}\")\n",
      "                    if not is_kenlm_available():\n",
      "                        logger.warning(\"Try to install `kenlm`: `pip install kenlm\")\n",
      "\n",
      "                    if not is_pyctcdecode_available():\n",
      "                        logger.warning(\"Try to install `pyctcdecode`: `pip install pyctcdecode\")\n",
      "\n",
      "    if task == \"translation\" and model.config.task_specific_params:\n",
      "        for key in model.config.task_specific_params:\n",
      "            if key.startswith(\"translation\"):\n",
      "                task = key\n",
      "                warnings.warn(\n",
      "                    f'\"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"{task}\"',\n",
      "                    UserWarning,\n",
      "                )\n",
      "                break\n",
      "\n",
      "    if tokenizer is not None:\n",
      "        kwargs[\"tokenizer\"] = tokenizer\n",
      "\n",
      "    if feature_extractor is not None:\n",
      "        kwargs[\"feature_extractor\"] = feature_extractor\n",
      "\n",
      "    if torch_dtype is not None:\n",
      "        kwargs[\"torch_dtype\"] = torch_dtype\n",
      "\n",
      "    if image_processor is not None:\n",
      "        kwargs[\"image_processor\"] = image_processor\n",
      "\n",
      "    if device is not None:\n",
      "        kwargs[\"device\"] = device\n",
      "\n",
      "    return pipeline_class(model=model, framework=framework, task=task, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(transformers.pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8b384-8e22-4eb0-82a9-a35b61481732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
