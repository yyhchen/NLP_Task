{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b0916a-b07e-48fd-9f29-60bcc57bddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e84e1c0-9b8c-4def-b964-33ffd896495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def load(\n",
      "    path: str,\n",
      "    config_name: Optional[str] = None,\n",
      "    module_type: Optional[str] = None,\n",
      "    process_id: int = 0,\n",
      "    num_process: int = 1,\n",
      "    cache_dir: Optional[str] = None,\n",
      "    experiment_id: Optional[str] = None,\n",
      "    keep_in_memory: bool = False,\n",
      "    download_config: Optional[DownloadConfig] = None,\n",
      "    download_mode: Optional[DownloadMode] = None,\n",
      "    revision: Optional[Union[str, Version]] = None,\n",
      "    **init_kwargs,\n",
      ") -> EvaluationModule:\n",
      "    \"\"\"Load a [`~evaluate.EvaluationModule`].\n",
      "\n",
      "    Args:\n",
      "\n",
      "        path (`str`):\n",
      "            Path to the evaluation processing script with the evaluation builder. Can be either:\n",
      "                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n",
      "                    e.g. `'./metrics/rouge'` or `'./metrics/rouge/rouge.py'`\n",
      "                - a evaluation module identifier on the HuggingFace evaluate repo e.g. `'rouge'` or `'bleu'` that are in either `'metrics/'`,\n",
      "                    `'comparisons/'`, or `'measurements/'` depending on the provided `module_type`\n",
      "        config_name (`str`, *optional*):\n",
      "            Selecting a configuration for the metric (e.g. the GLUE metric has a configuration for each subset).\n",
      "        module_type (`str`, default `'metric'`):\n",
      "            Type of evaluation module, can be one of `'metric'`, `'comparison'`, or `'measurement'`.\n",
      "        process_id (`int`, *optional*):\n",
      "            For distributed evaluation: id of the process.\n",
      "        num_process (`int`, *optional*):\n",
      "            For distributed evaluation: total number of processes.\n",
      "        cache_dir (`str`, *optional*):\n",
      "            Path to store the temporary predictions and references (default to `~/.cache/huggingface/evaluate/`).\n",
      "        experiment_id (`str`):\n",
      "            A specific experiment id. This is used if several distributed evaluations share the same file system.\n",
      "            This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n",
      "        keep_in_memory (`bool`):\n",
      "            Whether to store the temporary results in memory (defaults to `False`).\n",
      "        download_config ([`~evaluate.DownloadConfig`], *optional*):\n",
      "            Specific download configuration parameters.\n",
      "        download_mode ([`DownloadMode`], defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "            Download/generate mode.\n",
      "        revision (`Union[str, evaluate.Version]`, *optional*):\n",
      "            If specified, the module will be loaded from the datasets repository\n",
      "            at this version. By default it is set to the local version of the lib. Specifying a version that is different from\n",
      "            your local version of the lib might cause compatibility issues.\n",
      "\n",
      "    Returns:\n",
      "        [`evaluate.EvaluationModule`]\n",
      "\n",
      "    Example:\n",
      "\n",
      "        ```py\n",
      "        >>> from evaluate import load\n",
      "        >>> accuracy = load(\"accuracy\")\n",
      "        ```\n",
      "    \"\"\"\n",
      "    download_mode = DownloadMode(download_mode or DownloadMode.REUSE_DATASET_IF_EXISTS)\n",
      "    evaluation_module = evaluation_module_factory(\n",
      "        path, module_type=module_type, revision=revision, download_config=download_config, download_mode=download_mode\n",
      "    )\n",
      "    evaluation_cls = import_main_class(evaluation_module.module_path)\n",
      "    evaluation_instance = evaluation_cls(\n",
      "        config_name=config_name,\n",
      "        process_id=process_id,\n",
      "        num_process=num_process,\n",
      "        cache_dir=cache_dir,\n",
      "        keep_in_memory=keep_in_memory,\n",
      "        experiment_id=experiment_id,\n",
      "        hash=evaluation_module.hash,\n",
      "        **init_kwargs,\n",
      "    )\n",
      "\n",
      "    if module_type and module_type != evaluation_instance.module_type:\n",
      "        raise TypeError(\n",
      "            f\"No module of module type '{module_type}' not found for '{path}' locally, or on the Hugging Face Hub. Found module of module type '{evaluation_instance.module_type}' instead.\"\n",
      "        )\n",
      "\n",
      "    # Download and prepare resources for the metric\n",
      "    evaluation_instance.download_and_prepare(download_config=download_config)\n",
      "\n",
      "    return evaluation_instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rs = inspect.getsource(evaluate.load)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96178c6-bfeb-443e-be79-77a3bdd59db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
